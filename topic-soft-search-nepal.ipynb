{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymongo import MongoClient\n",
    "from bson.objectid import ObjectId\n",
    "from collections import OrderedDict\n",
    "import random\n",
    "import logging\n",
    "from datetime import datetime\n",
    "from functools import lru_cache\n",
    "from paths import data_path\n",
    "from pathlib import Path\n",
    "from gensim.models import KeyedVectors\n",
    "from tqdm import tqdm\n",
    "from pprint import pprint\n",
    "import spacy\n",
    "from nltk.corpus import stopwords\n",
    "import pyemd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from sklearn.cluster import KMeans, AgglomerativeClustering\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(format='%(asctime)s : %(message)s', level=logging.INFO)\n",
    "_info = logging.info\n",
    "\n",
    "client = MongoClient('mongodb://localhost:27017')\n",
    "db = client.twitter_news\n",
    "\n",
    "counters = dict()\n",
    "\n",
    "total_events = 3\n",
    "\n",
    "@lru_cache(maxsize=total_events)\n",
    "def get_representatives(event_id):\n",
    "    _info(\"getting representatives\")\n",
    "    representatives = db.representatives.find({'event': ObjectId(event_id)})\n",
    "    return list(representatives)\n",
    "\n",
    "\n",
    "@lru_cache(maxsize=total_events)\n",
    "def get_topics(event_id):\n",
    "    _info(\"getting topics\")\n",
    "    topics = list(db.topics.find({'event': ObjectId(event_id)}))\n",
    "    for t in topics:\n",
    "        if t['topic_name'] == \"Non relevant\":\n",
    "            comodin = t\n",
    "            topics.remove(t)\n",
    "            break\n",
    "    return topics, comodin\n",
    "\n",
    "\n",
    "@lru_cache(maxsize=1)\n",
    "def get_events():\n",
    "    _info(\"getting events\")\n",
    "    events = db.events.find()\n",
    "    return list(events)\n",
    "\n",
    "\n",
    "@lru_cache(maxsize=1)\n",
    "def get_tweets(a=None):\n",
    "    _info('getting all tweets')\n",
    "    all_tweets = db.tweets.find()\n",
    "    return list(all_tweets)\n",
    "\n",
    "\n",
    "@lru_cache(maxsize=3)\n",
    "def get_vectors(path):\n",
    "    _info(f\"loading fasttext vectors from {path}\")\n",
    "    word_vectors = KeyedVectors.load_word2vec_format(path)\n",
    "    return word_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm', tagger=False, entity=False, matcher=False)\n",
    "\n",
    "def hashtag_pipe(doc):\n",
    "    merged_hashtag = False\n",
    "    while True:\n",
    "        for token_index, token in enumerate(doc):\n",
    "            if token.text == '#':\n",
    "                if token.head is not None:\n",
    "                    start_index = token.idx\n",
    "                    end_index = start_index + len(token.head.text) + 1\n",
    "                    if doc.merge(start_index, end_index) is not None:\n",
    "                        merged_hashtag = True\n",
    "                        break\n",
    "        if not merged_hashtag:\n",
    "            break\n",
    "        merged_hashtag = False\n",
    "    return doc\n",
    "\n",
    "nlp.add_pipe(hashtag_pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-06-20 18:07:15,638 : loading fasttext vectors from /home/mquezada/anchor-text-twitter/data/ft_alltweets_model.vec\n",
      "2018-06-20 18:07:15,641 : loading projection weights from /home/mquezada/anchor-text-twitter/data/ft_alltweets_model.vec\n",
      "2018-06-20 18:08:18,772 : loaded (1076139, 100) matrix from /home/mquezada/anchor-text-twitter/data/ft_alltweets_model.vec\n"
     ]
    }
   ],
   "source": [
    "path = data_path / Path('/home/mquezada/anchor-text-twitter/data/ft_alltweets_model.vec')\n",
    "ft_comp = get_vectors(path.as_posix())\n",
    "\n",
    "#path = data_path / Path('/home/mquezada/anchor-text-twitter/data/ft_all_tweets_1line1tweet.vec')\n",
    "#ft_all = get_vectors(path.as_posix())\n",
    "\n",
    "#path = data_path / Path('/home/mquezada/anchor-text-twitter/data/w2v_all_tweets.txt')\n",
    "#w2v_all = get_vectors(path.as_posix())\n",
    "\n",
    "#path = data_path / Path('/home/mquezada/anchor-text-twitter/data/w2v_all_tweets_1_line_1_component.txt')\n",
    "#w2v_comp = get_vectors(path.as_posix())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ft_comp\n",
      "0.33911799442295903\n",
      "\n",
      "ft_all\n",
      "0.3300593981594927\n",
      "\n",
      "w2v_comp\n",
      "0.09744087254883523\n",
      "\n",
      "w2v_all\n",
      "0.1679580041090767\n"
     ]
    }
   ],
   "source": [
    "w = 'iphone'\n",
    "w2 = 'earthquake'\n",
    "\n",
    "print(\"ft_comp\")\n",
    "pprint(ft_comp.similarity(w, w2))\n",
    "print()\n",
    "print(\"ft_all\")\n",
    "pprint(ft_all.similarity(w, w2))\n",
    "print()\n",
    "print(\"w2v_comp\")\n",
    "pprint(w2v_comp.similarity(w, w2))\n",
    "print()\n",
    "print(\"w2v_all\")\n",
    "pprint(w2v_all.similarity(w, w2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-06-20 18:08:30,645 : getting events\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'_id': ObjectId('5b171725da870923dcb0478f'),\n",
       "  'event_name': 'libya_hotel_tweets.tsv',\n",
       "  'description': 'In January 2015, the Corinthia Hotel in Tripoli was attacked by men affiliated with the Islamic State of Iraq and the Levant (ISIL). The hotel was popular with foreign officials and government workers; it had previously housed the Libyan Prime Minister.',\n",
       "  'human_name': '2015 Corinthia Hotel attack'},\n",
       " {'_id': ObjectId('5b171726da870923dcb04790'),\n",
       "  'event_name': 'oscar_pistorius_tweets.tsv',\n",
       "  'description': 'The trial of Oscar Pistorius for the murder of Reeva Steenkamp and several gun-related charges (The State vs Oscar Pistorius) in the High Court of South Africa in Pretoria opened on 3 March 2014.',\n",
       "  'human_name': 'Trial of Oscar Pistorius'},\n",
       " {'_id': ObjectId('5b171726da870923dcb04791'),\n",
       "  'event_name': 'nepal_tweets.tsv',\n",
       "  'description': 'The April 2015 Nepal earthquake (also known as the Gorkha earthquake)[5][8] killed nearly 9,000 people and injured nearly 22,000. It occurred at 11:56 Nepal Standard Time on 25 April, with a magnitude of 7.8Mw or 8.1Ms and a maximum Mercalli Intensity of VIII (Severe)',\n",
       "  'human_name': 'April 2015 Nepal earthquake'}]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "events = get_events()\n",
    "events"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Obtener los tweets del evento y crear mapa de representantes-tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-06-20 18:08:37,981 : getting all tweets\n",
      "2018-06-20 18:08:43,184 : getting representatives\n",
      "2018-06-20 18:08:43,580 : getting topics\n",
      "100%|██████████| 642251/642251 [00:00<00:00, 1182423.46it/s]\n"
     ]
    }
   ],
   "source": [
    "all_tweets = get_tweets()\n",
    "events = get_events()\n",
    "event = events[2]\n",
    "\n",
    "representatives = get_representatives(event['_id'])\n",
    "topics, _ = get_topics(event['_id'])\n",
    "\n",
    "rep_tweet = dict()\n",
    "for t in tqdm(all_tweets):\n",
    "    rep_tweet[t['representative']] = t\n",
    "\n",
    "rep_set = set([r['_id'] for r in representatives])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "144837"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# asociacion entre representativos y tweets\n",
    "\n",
    "tweets_this_event = [t for r, t in rep_tweet.items() if r in rep_set]\n",
    "len(tweets_this_event)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Listar topicos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avalanche in Mount Everest\n",
      "Death tolls\n",
      "Reports on the magnitude of the earthquake\n",
      "Rescue of people\n",
      "Ways to help\n",
      "International aid\n",
      "Destruction of historical buildings\n",
      "Humanitarian crisis\n",
      "Destruction of buildings\n",
      "Replicas of the earthquake\n"
     ]
    }
   ],
   "source": [
    "for topic in topics:\n",
    "    print(topic['topic_name'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generar tokens por cada tweet en `tweet_tokens`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 144837/144837 [07:42<00:00, 313.39it/s]\n"
     ]
    }
   ],
   "source": [
    "# gen tokens for each tweet in the event\n",
    "\n",
    "tweets_tokens = set()\n",
    "for doc in tqdm(nlp.pipe(map(lambda t: t['text'], tweets_this_event), n_threads=8, batch_size=1024), \n",
    "                total=len(tweets_this_event)):\n",
    "    \n",
    "    tokens = frozenset([token.lower_ \n",
    "                        for token in doc \n",
    "                        if token.lower_ not in stopwords.words('english') and token.lower_ in ft_comp])\n",
    "    \n",
    "    if tokens: \n",
    "        tweets_tokens.add(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'_id': ObjectId('5b1717abda870923dcbdc47c'),\n",
       " 'tweet_id': 592008412004798464,\n",
       " 'text': 'Nepal was hit with a 7.9 earthquake. Over 1000 people have lost their lives. If you can, please consider donating http://t.co/MAUEatbNRs',\n",
       " 'created_at': datetime.datetime(2015, 4, 25, 16, 52, 59),\n",
       " 'retweet_id': None,\n",
       " 'reply_id': None,\n",
       " 'short_urls': ['http://t.co/MAUEatbNRs'],\n",
       " 'expanded_urls': ['https://www.globalgiving.org/projects/nepal-earthquake-relief-fund/'],\n",
       " 'representative': ObjectId('5b1717abda870923dcbdc47b')}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sim2(tokens_a, tokens_b):\n",
    "    return ft_comp.n_similarity(tokens_a, tokens_b)\n",
    "\n",
    "def mmr(docs, q, lambda_, sim):\n",
    "    selected = OrderedDict()\n",
    "    while set(selected) != docs:\n",
    "        remaining = docs - set(selected)\n",
    "        mmr_score = lambda x: lambda_ * sim(x, q) - (1 - lambda_) * max([sim(x, y) for y in set(selected) - {x}] or [0])\n",
    "        next_selected = argmax(remaining, mmr_score)\n",
    "        selected[next_selected] = len(selected)\n",
    "        yield selected   \n",
    "\n",
    "def argmax(keys, f):\n",
    "    return max(keys, key=f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'in', 'avalanche', 'mount', 'everest'}\n",
      "\n",
      "avalanche unleashed everest news mount earthquake\n",
      "avalanche mount everest nepal\n",
      "avalanche british climber everest help mount appeals\n",
      "reports climbers everest mount nepalearthquake\n",
      "avalanche climber aftermath everest region describes mount watch\n",
      "avalanche deadly triggers everest #nepalearthquake mount via\n",
      "climbing hit avalanche mount everest would like earthquake\n",
      "climbing avalanche nepal quake everest survives triggered man mount wisconsin\n",
      "avalanche killed everest 8 mount earthquake\n",
      "climbers goes nepal everest heart mount\n",
      "\n",
      "\n",
      "{'death', 'tolls'}\n",
      "\n",
      "nepal toll death sad rising\n",
      "1,000 nepal tolls 's heartbreaking death\n",
      "death rises toll\n",
      "story updated toll reflect increased death\n",
      "continues nepal toll rise death\n",
      "continues nepal toll horrible people increase death thoughts\n",
      "1,200 goes nepal toll people heart exceeds death\n",
      "goes toll nepal heart death rising\n",
      "nepal toll exponentially death rising\n",
      "alarming nepal toll rate hour death sad rising\n",
      "\n",
      "\n",
      "{'reports', 'on', 'of', 'magnitude', 'the', 'earthquake'}\n",
      "\n",
      "powerful struck world hits nepal quake 7.5 les revolt major near 's capital news magnitude earthquake\n",
      "reports updated felt report photos graphics news earthquake\n",
      "strikes nepal damage video 7.9-magnitude rt news reported breaking earthquake\n",
      "ring nepal weird outside occured fire bit 's region magnitude earthquake\n",
      "feels india read aftershocks massive northern breaking earthquake\n",
      "struck streaming india causing deaths nepal quake live 7.9 coverage video multiple online near watch magnitude earthquake\n",
      "strikes nepal one 7.9 initially reported via bad looks magnitude like earthquake\n",
      "powerful india media magnitude strikes earthquake 7.9 foreign #nepal agencies amp parts news reported confirmed\n",
      "reports wishing magnitude #earthquake nepal updating people news constantly safety\n",
      "struck geological reports nepal injuries damage word reported 7.5 u.s. survey magnitude breaking earthquake\n",
      "\n",
      "\n",
      "{'of', 'rescue', 'people'}\n",
      "\n",
      "still needs rubble trapped urgent operations international people help definitely assistance rescue many #nepal\n",
      "thought nepal workers victims praying people heartbreaking rescue\n",
      "still going recovery nepal dead tragic people 1,300 news rescue operations\n",
      "1,000 save killed nepal order people almost mission ongoing lives rescue earthquake\n",
      "thousands rip today rubble nepal possible trapped lost lives hopeful rescue beneath also\n",
      "least state killed said 1,500 continue country people teams 's surviv rescue search televison\n",
      "rubble- hard inside nepal locals trapped people rt breaking trying rescue now---\n",
      "giving helping hope nepal victims thanks salute soon soldiers works hands rescued\n",
      "still around need nepal dead report serious 700 peoples rescue many incomplete\n",
      "wake workers praying people well amp aid #nepalearthquake rescue #nepal\n",
      "\n",
      "\n",
      "{'ways', 'to', 'help'}\n",
      "\n",
      "might help\n",
      "#news victims ways #nepalquake help list looking\n",
      "friendly user get creative enhance people must way benefitted idea could say\n",
      "us right need please reach help #nepalearthquake\n",
      "find share google started people ones beloved help #nepalearthquake initiative\n",
      "us pray try quake possible let victims ways help\n",
      "locate find dear one may rt amp guys help\n",
      "stoke us hope let help bring\n",
      "need together people come help list #nepalearthquake donation options wartime\n",
      "lost people help might find\n",
      "\n",
      "\n",
      "{'aid', 'international'}\n",
      "\n",
      "team us 1 agency disaster development nepal $ million sending dart international #nepalquake response aid earmarked\n",
      "state government emergency international declared humanitarian assistance nepalese appealed\n",
      "support needed international\n",
      "kathmandu crescent federation international cross breakingnews red\n",
      "strategy forces international rather best aid deal prevention nature\n",
      "rush nepal quake victims international reach #civil_vision aid groups\n",
      "disaster charities emergency international two teams amp response relief w/\n",
      "victims emergency preparing help aid israel earthquake send\n",
      "begin nepal emergency international teams response news handicap earthquake\n",
      "emergencies offers nepal russia tass ministry 's aid eliminating earthquake aftermaths\n",
      "\n",
      "\n",
      "{'of', 'historical', 'buildings', 'destruction'}\n",
      "\n",
      "archaeologically important nepal historic rt structures monuments earthquake destroys\n",
      "st lightning earthquake damage strike policy fire amp buildings covers riots malicious contents explosion\n",
      "difficult stories destruction extent imagine massive\n",
      "weapons poorly destruction become mass constructed place buildings quakes like #nepal\n",
      "images ruins temples destruction artifacts sacred w grand horrific amp along religious #prayers death #nepal\n",
      "ancient nepal loss distraction major monuments reported life\n",
      "images destroyed nepal historical international communities help buildings come earthquake shocking\n",
      "firsthand destruction accounts destructio via\n",
      "world destroyed temples nepal historical including unesco landmarks many earthquake heritage\n",
      "devastating wow ancient destruction earthquake coming levelled brick photos wood buildings many #nepal\n",
      "\n",
      "\n",
      "{'humanitarian', 'crisis'}\n",
      "\n",
      "nepal earthquake 's humanitarian crisis\n",
      "needs experts situation nepal eu possible + assessing monitoring closely asia aid #kathmandu humanitarian brussels\n",
      "support immediately nepal natural victims pledged crises humanitarian contribute\n",
      "within hours nepal eu victims sends aid humanitarian earthquake\n",
      "partner nepal oxfam #utvpol strategic 's response humanitarian earthquake\n",
      "state government emergency international declared humanitarian assistance nepalese appealed\n",
      "continues evac spearheading nepal #india diplomacy yemen aggressive relief humanitarian earthquake\n",
      "support ground crisis nepal providing help humanitarian following earthquake\n",
      "nepal humanitarian #nepalearthquake massive crisis\n",
      "uk experts nepal bbc sends news humanitarian earthquake\n",
      "\n",
      "\n",
      "{'of', 'buildings', 'destruction'}\n",
      "\n",
      "devastating wow ancient destruction earthquake coming levelled brick photos wood buildings many #nepal\n",
      "reports nepal coming buildings heritage\n",
      "firsthand destruction accounts destructio via\n",
      "st lightning earthquake damage strike policy fire amp buildings covers riots malicious contents explosion\n",
      "world iconic neighboring loss country nepal\" buildings massive sites heritage\n",
      "ancient nepal loss distraction major monuments reported life\n",
      "old nepal deadly levels homes photos buildings collapses centuries earthquake\n",
      "due destroyed many nepal loss got building places demolished roads monuments massive earthquake collapsed\n",
      "ancient kathmandu majority destruction nepal old eartquake buildings city pronounced dilapidated\n",
      "archaeologically important nepal historic structures monuments earthquake destroys\n",
      "\n",
      "\n",
      "{'earthquake', 'of', 'the', 'replicas'}\n",
      "\n",
      "another going nepal kinetic plates sign tectonic 's earth probably massive earthquake\n",
      "rocks powerful nepal dead leaving 100s photos earth heartbreaking see earthquake strong\n",
      "show stand first buildings earthquake\n",
      "nepal cousin april cam demasiado fuckup 's security 25 footage place earthquake esto\n",
      "ever rocks southern north largest recorded 9.2 alaska portion american 1964 magnitude massive earthquake\n",
      "actual right falling post sim picture buildings city add earthquake\n",
      "first thought nepal study earthquake #geology new case volcano 's chile head ohh sad entered\n",
      "look living nepal praying scale map see earthquake population\n",
      "rolled power doorway stone nt massive earthquake life tomb\n",
      "camera shaking nepal video liked security earthquake insane\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lambda_ = 0.7\n",
    "\n",
    "for topic in topics:\n",
    "    query = set(map(lambda s: s.lower(), topic['topic_name'].split()))\n",
    "\n",
    "    print(query)\n",
    "    print()\n",
    "    \n",
    "    i = 0\n",
    "    for doc in mmr(tweets_tokens, query, lambda_, sim2):\n",
    "        print(' '.join(list(doc.items())[-1][0]))\n",
    "        i += 1\n",
    "        if i == 10:\n",
    "            break\n",
    "    \n",
    "    print()\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(frozenset({'#kabari', 'coming'}), 9)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(doc.items())[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El problema con lo siguiente son los topicos: las palabras definen los tweets que se van a obtener con la búsqueda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['gunmen', 'storm', 'luxury', 'hotel', 'libya', 'capital', 'least', '3', 'killed', 'hotel', 'popular', 'foreigners', 'c', '#muhamadjabal']\n",
      "\n",
      "['car', 'bomb', 'explodes']\n",
      "0.6755222282719202\n",
      "\n",
      "['isis', 'adjudicates', 'attack']\n",
      "0.6617886776347398\n",
      "\n",
      "['report', 'amount', 'casualties']\n",
      "0.6782251905755576\n",
      "\n",
      "['hostages', 'taken']\n",
      "0.7047494552877931\n",
      "\n",
      "['report', 'number', 'attackers']\n",
      "0.7330240186949215\n",
      "\n",
      "['confrontation', 'security', 'forces']\n",
      "0.6800127367584079\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# convertir topicos en word vectors\n",
    "# convertir tweets del evento (representativos) en vectores\n",
    "## app lowercase\n",
    "## app tokenize\n",
    "## del urls\n",
    "# computar similitud entre topico y tweet\n",
    "\n",
    "rnd_tweet = random.choice(tweets_this_event)\n",
    "text = nlp(rnd_tweet['text'])\n",
    "\n",
    "tweet_tokens = []\n",
    "\n",
    "for token in text:\n",
    "    if token.lower_ in stopwords.words('english') or token.lower_ not in vectors:\n",
    "        continue\n",
    "    tweet_tokens.append(token.lower_)\n",
    "\n",
    "topic_tokens = []\n",
    "for topic in nlp.pipe([t['topic_name'] for t in topics]):\n",
    "    topic_tokens.append([token.lower_ for token in topic \n",
    "                         if token.lower_ in vectors and token.lower_ not in stopwords.words('english')])\n",
    "    \n",
    "print(tweet_tokens)\n",
    "print()\n",
    "for tokens in topic_tokens:\n",
    "    print(tokens)\n",
    "    print(vectors.n_similarity(tweet_tokens, tokens))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Idea: hacer clustering de palabras del vocabulario y samplear tweets a partir de los clusters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14233/14233 [01:48<00:00, 130.97it/s]\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(preprocessor=lambda x: x, tokenizer=lambda x: x)\n",
    "\n",
    "m = vectorizer.fit_transform(tweets_tokens)\n",
    "m2 = m.transpose()  # represent words instead of documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(3, 7784), (6, 6), (4, 5), (13, 5), (12, 5), (14, 5), (1, 5), (2, 4), (17, 4), (15, 4), (19, 3), (18, 2), (16, 2), (0, 2), (9, 1), (8, 1), (11, 1), (7, 1), (10, 1), (5, 1)]\n",
      "\n",
      "['pic', 'trans']\n",
      "\n",
      "['kill', 'killed', 'news', 'says', 'security']\n",
      "\n",
      "['3', 'guards', 'hostages', 'take']\n",
      "\n",
      "[\"'s\", 'bomb', 'car', 'explodes', 'outside']\n",
      "\n",
      "['tripoli']\n",
      "\n",
      "['foreigner', 'linked', 'militant', 'popular', 'possibly', 'top']\n",
      "\n",
      "['gunmen']\n",
      "\n",
      "['capital']\n",
      "\n",
      "['attack']\n",
      "\n",
      "['libyan']\n",
      "\n",
      "['design']\n",
      "\n",
      "['corinthia', 'luxurious', 'shot', 'tuesday', 'way']\n",
      "\n",
      "['affiliate', 'assault', 'behind', 'islamic', 'state']\n",
      "\n",
      "['dead', 'eight', 'least', 'official', 'storm']\n",
      "\n",
      "['exclusive', 'offers', 'special', 'visa']\n",
      "\n",
      "['group', 'isis']\n",
      "\n",
      "['businessweek', 'ei', 'killing', 'militants']\n",
      "\n",
      "['5', 'foreigners']\n",
      "\n",
      "['hotel', 'libya', 'luxury']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "all_words = np.array(vectorizer.get_feature_names())\n",
    "\n",
    "nc = 20\n",
    "km = KMeans(n_clusters=nc, n_jobs=-1, n_init=30, max_iter=500)\n",
    "km.fit(m2)\n",
    "\n",
    "print(Counter(km.labels_).most_common(nc))\n",
    "print()\n",
    "\n",
    "for i in range(nc):\n",
    "    words = all_words[km.labels_ == i].tolist()\n",
    "    if len(words) < 20:\n",
    "        pprint(words)\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "agglomerative clustering doesn't work well in this case:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AgglomerativeClustering(affinity='cosine', compute_full_tree='auto',\n",
       "            connectivity=None, linkage='complete', memory='/tmp',\n",
       "            n_clusters=20, pooling_func=<function mean at 0x7f6711ed8ae8>)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agg = AgglomerativeClustering(n_clusters=20, affinity=\"cosine\", memory=\"/tmp\", linkage=\"complete\")\n",
    "\n",
    "agg.fit(m2.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['#vofnafrica', 'dictator', 'moam', 'ouster', '|']\n",
      "\n",
      "['gulbenkian', 'km', 'lisbon', 'located', 'mi', 'rios', 'sete', 'within', 'zoo']\n",
      "\n",
      "['#yugvani', 'claim', 'zee']\n",
      "\n",
      "['appears', 'reveal', '~10:00']\n",
      "\n",
      "['art', 'dolder', 'incomparable', 'zurich']\n",
      "\n",
      "['african', 'news24', '|via']\n",
      "\n",
      "['blaze', '~via']\n",
      "\n",
      "['#architecture', 'zermatt']\n",
      "\n",
      "['girls', 'menara', 'ze']\n",
      "\n",
      "['zzzzzz']\n",
      "\n",
      "['~inhabitat']\n",
      "\n",
      "['ali', 'd.', 'david', 'kirkpatrick', 'suliman', 'zway']\n",
      "\n",
      "['bbcnews', '|thedailypr']\n",
      "\n",
      "['#eventsus', '#ticket', '12,500', 'nfl', 'xlix', 'zone']\n",
      "\n",
      "['bachelor', 'gadgets', '|the']\n",
      "\n",
      "['happens', 'zokmed']\n",
      "\n",
      "['baca', 'lt;-', 'selengkapnya', '|news|=']\n",
      "\n",
      "['zealand']\n",
      "\n",
      "['||']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(20):\n",
    "    words = all_words[agg.labels_ == i].tolist()\n",
    "    if len(words) < 50:\n",
    "        pprint(words)\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-06-15 12:32:58,021 : precomputing L2-norms of word weight vectors\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('saintiago', 0.8565769791603088),\n",
       " ('#santiago', 0.841245174407959),\n",
       " ('santia', 0.8093695640563965),\n",
       " ('nacional', 0.7871439456939697),\n",
       " ('#ripsantiago', 0.7835432291030884),\n",
       " ('cristobal', 0.7817075252532959),\n",
       " ('chilea', 0.7602672576904297),\n",
       " ('santo', 0.7583289742469788),\n",
       " ('pestanosantiago', 0.7576349377632141),\n",
       " ('santiam', 0.7572433948516846),\n",
       " ('chilean', 0.7534645795822144),\n",
       " ('chileans', 0.7500560283660889),\n",
       " ('vicente', 0.7482686042785645),\n",
       " ('neuquen', 0.7467107176780701),\n",
       " ('pacheco', 0.7450253963470459),\n",
       " ('centro', 0.7428617477416992),\n",
       " ('penitencia', 0.7422252297401428),\n",
       " ('#antoniosantiago', 0.7407231330871582),\n",
       " ('monterrey', 0.7407140731811523),\n",
       " ('#erupcionvillarrica', 0.7407123446464539),\n",
       " ('agencia', 0.7366681694984436),\n",
       " ('#santiagobernabeu', 0.7362847328186035),\n",
       " ('libertadores', 0.7348600625991821),\n",
       " ('camino', 0.733985185623169),\n",
       " ('santos', 0.7329348921775818),\n",
       " ('ensenada', 0.732906699180603),\n",
       " ('chileing', 0.7327314615249634),\n",
       " ('strano', 0.7322192192077637),\n",
       " ('caraballo', 0.7308638691902161),\n",
       " ('reddeemergencia', 0.7296812534332275),\n",
       " ('davao', 0.7268149852752686),\n",
       " ('araucania', 0.7258568406105042),\n",
       " ('jorge', 0.7258443832397461),\n",
       " ('chile#spain', 0.7223672866821289),\n",
       " ('villarre', 0.721317708492279),\n",
       " ('perudate', 0.7212801575660706),\n",
       " ('cortes', 0.7210659980773926),\n",
       " ('santino', 0.7197098135948181),\n",
       " ('canteras', 0.7191683053970337),\n",
       " ('#santiagodecuba', 0.7185970544815063),\n",
       " ('montesion', 0.7182221412658691),\n",
       " ('stratovolcano', 0.717942476272583),\n",
       " ('aguado', 0.7172447443008423),\n",
       " ('guatema', 0.7169990539550781),\n",
       " ('iaquinta', 0.7169511318206787),\n",
       " ('pasillo', 0.7166237235069275),\n",
       " ('villarruel', 0.7165982127189636),\n",
       " ('laquinta', 0.7159038782119751),\n",
       " ('campo', 0.7154406309127808),\n",
       " ('montt', 0.7150738835334778),\n",
       " ('colombiana', 0.7146137952804565),\n",
       " ('santaquin', 0.7144999504089355),\n",
       " ('chile[gallery', 0.713274359703064),\n",
       " ('perurena', 0.713173508644104),\n",
       " ('excepcional', 0.7126935720443726),\n",
       " ('almaden', 0.7126885652542114),\n",
       " ('santisimo', 0.7126781344413757),\n",
       " ('arribart', 0.7125349640846252),\n",
       " ('atonchile', 0.7109968066215515),\n",
       " ('pacifico', 0.7107242345809937),\n",
       " ('camilo', 0.7104405164718628),\n",
       " ('rocuant', 0.7100630402565002),\n",
       " ('colima', 0.7098715305328369),\n",
       " ('casilla', 0.7091021537780762),\n",
       " ('-chilean', 0.7087603211402893),\n",
       " ('madero', 0.7084847688674927),\n",
       " ('guatamalan', 0.7084778547286987),\n",
       " ('veracruz', 0.7082388997077942),\n",
       " ('monteros', 0.7081698179244995),\n",
       " ('chaparros', 0.7081633806228638),\n",
       " ('agenciauno', 0.7076833248138428),\n",
       " ('puert', 0.7074266076087952),\n",
       " ('marica', 0.7071958780288696),\n",
       " ('aguardo', 0.706601619720459),\n",
       " ('nacion', 0.7065248489379883),\n",
       " ('santol', 0.706398606300354),\n",
       " ('mendocino', 0.7058477401733398),\n",
       " ('puerta', 0.7057089805603027),\n",
       " ('colombianas', 0.7054761052131653),\n",
       " ('tiago', 0.7048583030700684),\n",
       " ('bragantino', 0.7045746445655823),\n",
       " ('residencia', 0.7039587497711182),\n",
       " ('plata', 0.7039375305175781),\n",
       " ('paranaense', 0.7037484645843506),\n",
       " ('aracataca', 0.7030707597732544),\n",
       " ('florencia', 0.7028565406799316),\n",
       " ('caraga', 0.7026294469833374),\n",
       " ('rosarito', 0.7023622989654541),\n",
       " ('guatama', 0.7022905349731445),\n",
       " ('paca', 0.70200514793396),\n",
       " ('chile.#spainvschile', 0.7005723118782043),\n",
       " ('chiled', 0.6997271776199341),\n",
       " ('#chileans', 0.6997028589248657),\n",
       " ('placencia', 0.6996564865112305),\n",
       " ('universidade', 0.6995754241943359),\n",
       " ('miraflores', 0.6993354558944702),\n",
       " ('bello', 0.699242115020752),\n",
       " ('olas', 0.6989561319351196),\n",
       " ('ernesto', 0.6989526748657227),\n",
       " ('corona', 0.6988338232040405)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectors.similar_by_word('santiago', topn=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
