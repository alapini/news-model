{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieval tool\n",
    "\n",
    "`~/CrisisMMD/TweetsRetrievalTool-v2.0`\n",
    "\n",
    "two files in the dir: tweet_ids.txt, twitter.properties\n",
    "\n",
    "exec `java -classpath TweetsRetrievalTool-2.0.jar qa.org.qcri.tweetsretrievaltool.TweetsRetrievalTool`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Irma Hurricane\n",
    "\n",
    "```\n",
    "1418517 tweet2.json\n",
    "1700788 tweet_ids.txt\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import url_expander\n",
    "import spacy\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import requests\n",
    "import multiprocessing\n",
    "import time\n",
    "import datetime\n",
    "import asyncio\n",
    "import aiohttp\n",
    "from typing import List\n",
    "import logging\n",
    "import os\n",
    "from collections import namedtuple\n",
    "import re\n",
    "from unionfind import UnionFind\n",
    "import numpy as np\n",
    "from gensim.models import KeyedVectors\n",
    "import pickle as pkl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resolve URLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base = Path('/home/mquezada/CrisisMMD/full_tweets/data/tweet_ids_v1/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en', disable=[\"tagger\", \"parser\", \"ner\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "texts = []\n",
    "\n",
    "with (base / Path('irma_tweets.json')).open() as f:\n",
    "    for line in tqdm(f):\n",
    "        tw = json.loads(line)\n",
    "        texts.append(tw['full_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "urls = []\n",
    "\n",
    "for text in tqdm(nlp.pipe(texts, n_threads=-1, batch_size=10000), total=len(texts)):\n",
    "    for token in text:\n",
    "        if token.like_url:\n",
    "            urls.append(token.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls_path = base / Path('irma_urls.txt')\n",
    "with urls_path.open('w') as f:\n",
    "    for u in urls:\n",
    "        f.write(u + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#logging.basicConfig(format='%(asctime)s - %(levelname)s - %(name)s - %(message)s', level=logging.ERROR)\n",
    "\n",
    "def resolve_url(short_url):\n",
    "    try:\n",
    "        response = requests.head(short_url, allow_redirects=True)\n",
    "        #logging.info('Done: {} ({})'.format(response.url, response.status_code))\n",
    "        pid =  os.getpid()\n",
    "        \n",
    "        with (base / Path(f'resolved_urls/urls_full_{pid}.txt')).open('a') as f:\n",
    "            f.write(f'{short_url}\\t{response.url}\\t{response.status_code}\\n')\n",
    "            \n",
    "        return short_url, response.url, response.status_code\n",
    "    \n",
    "    except Exception as e:\n",
    "        logging.error(str(e))\n",
    "    \n",
    "    return short_url, None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "\n",
    "p = multiprocessing.Pool(processes=20)\n",
    "result = p.map(resolve_url, urls)\n",
    "#p.terminate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load URLs and Tweets for Irma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "385055"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_urls = Path('/home/mquezada/CrisisMMD/full_tweets/data/tweet_ids_v1/resolved_urls/').glob('*.txt')\n",
    "\n",
    "short_expanded = dict()\n",
    "\n",
    "for fn in base_urls:\n",
    "    with fn.open() as f:\n",
    "        for line in f:\n",
    "            short, expanded, _ = line.split('\\t')\n",
    "            short_expanded[short] = expanded\n",
    "            \n",
    "len(short_expanded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tweet = namedtuple('Tweet', 'tweet_id retweet_id quote_id reply_id short_urls expanded_urls text created_at')\n",
    "url_re = re.compile(r'(https?://t.co/[a-zA-Z0-9]+)')\n",
    "hashtag_re = re.compile(r'(#[a-zA-Z0-9]+)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1418517/1418517 [02:06<00:00, 11212.43it/s]\n"
     ]
    }
   ],
   "source": [
    "base_tweet = Path('/home/mquezada/CrisisMMD/full_tweets/data/tweet_ids_v1/irma_tweets.json')\n",
    "\n",
    "event_data = dict()\n",
    "ignored_amount = 0\n",
    "missing_urls = 0\n",
    "\n",
    "with base_tweet.open() as f:\n",
    "    for line in tqdm(f, total=1418517):\n",
    "        t = json.loads(line)\n",
    "        text = t['full_text']\n",
    "        \n",
    "        urls_in_tweet = url_re.findall(text)\n",
    "        n_hashtags = hashtag_re.findall(text)\n",
    "        \n",
    "        if len(n_hashtags) >= 4 or len(urls_in_tweet) >= 3:\n",
    "            ignored_amount += 1\n",
    "            continue\n",
    "            \n",
    "        expanded_map = dict()\n",
    "        for i, short_url in enumerate(urls_in_tweet):\n",
    "            expanded_url = short_expanded.get(short_url)\n",
    "            expanded_map[i] = expanded_url\n",
    "            \n",
    "            if not expanded_url:\n",
    "                missing_urls += 1            \n",
    "        \n",
    "        rt = t.get('retweeted_status')\n",
    "        if rt:\n",
    "            rt = rt.get('id')\n",
    "        qt = t.get('quoted_status_id')\n",
    "        rp = t.get('in_reply_to_status_id')        \n",
    "        \n",
    "        tweet = Tweet(\n",
    "            str(t['id']),\n",
    "            str(rt),\n",
    "            str(qt),\n",
    "            str(rp),\n",
    "            urls_in_tweet,\n",
    "            expanded_map,\n",
    "            text,\n",
    "            t['created_at']\n",
    "        )\n",
    "        event_data[tweet.tweet_id] = tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tweets 1372366\n",
      "missing urls 52178\n",
      "ignored 46151\n"
     ]
    }
   ],
   "source": [
    "print('tweets', len(event_data))\n",
    "print('missing urls', missing_urls)\n",
    "print('ignored', ignored_amount)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-10-29 16:42:02,420 : create pairs (t, u) or (t, t') for each tweet t and url u or replied/retweeted tweet t'\n",
      "2018-10-29 16:42:03,417 : total pairs: 819914, retweets: 960528, quotes: 427409, replies: 41154 (missing: 37042)\n",
      "2018-10-29 16:42:03,417 : applying union-find\n",
      "2018-10-29 16:43:47,119 : total components: 198912\n"
     ]
    }
   ],
   "source": [
    "logging.basicConfig(format='%(asctime)s : %(message)s', level=logging.INFO)\n",
    "_info = logging.info\n",
    "\n",
    "def gen_model():    \n",
    "    _info(\"create pairs (t, u) or (t, t') for each tweet t and url u or replied/retweeted tweet t'\")\n",
    "    replies_amount = 0\n",
    "    retweets_amount = 0\n",
    "    quotes_amount = 0\n",
    "    missing_replies_amount = 0\n",
    "    pairs = []\n",
    "    for tweet_id, tweet in event_data.items():\n",
    "        [pairs.append((tweet_id, url)) for url in tweet.expanded_urls.values() if url]\n",
    "\n",
    "        # retweets ARE considered, due to be exact text copies of the retweeted tweet\n",
    "        if tweet.retweet_id != 'None':\n",
    "            retweets_amount += 1\n",
    "        if tweet.quote_id != 'None':\n",
    "            quotes_amount += 1\n",
    "        if tweet.reply_id != 'None':\n",
    "            replies_amount += 1\n",
    "            if tweet.reply_id in event_data:\n",
    "                pairs.append((tweet_id, tweet.reply_id))\n",
    "            else:\n",
    "                missing_replies_amount += 1\n",
    "    _info(f'total pairs: {len(pairs)}, retweets: {retweets_amount}, quotes: {quotes_amount}, replies: {replies_amount} '\n",
    "          f'(missing: {missing_replies_amount})')\n",
    "\n",
    "    ##########\n",
    "\n",
    "    \"\"\"\n",
    "        all keys must be the same time (in this case, strings);\n",
    "        unionfind will vectorize operations and will cast everything in the array to the same type,\n",
    "        so if there are integers and strings, it will cast everything to string and comparisons will fail\n",
    "        when calling uf.components().\n",
    "    \"\"\"\n",
    "\n",
    "    _info('applying union-find')\n",
    "    uf = UnionFind()\n",
    "    for u, v in pairs:\n",
    "        uf.union(u, v)\n",
    "    _info(f'total components: {len(uf.components())}')\n",
    "\n",
    "    return uf\n",
    "\n",
    "uf = gen_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "components = uf.components()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-10-29 16:49:46,874 : loading projection weights from /home/mquezada/anchor-text-twitter/data/ft_alltweets_model.vec\n",
      "2018-10-29 16:50:46,685 : loaded (1076139, 100) matrix from /home/mquezada/anchor-text-twitter/data/ft_alltweets_model.vec\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load('en', disable=[\"tagger\", \"parser\", \"ner\"])\n",
    "we = KeyedVectors.load_word2vec_format('/home/mquezada/anchor-text-twitter/data/ft_alltweets_model.vec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 198912/198912 [00:00<00:00, 224046.50it/s]\n"
     ]
    }
   ],
   "source": [
    "docs = dict()\n",
    "\n",
    "for component in tqdm(components):\n",
    "    comp_key = []\n",
    "    comp_ids = []\n",
    "    \n",
    "    for elem in component:\n",
    "        if elem.startswith('http'):\n",
    "            comp_key.append(elem)\n",
    "        else:\n",
    "            t = event_data.get(elem)\n",
    "            if not t:\n",
    "                print(\"err\")\n",
    "            comp_ids.append(t.tweet_id)\n",
    "\n",
    "    # component does not have url\n",
    "    if not comp_key:\n",
    "        comp_key.append(np.random.choice(comp_ids))\n",
    "\n",
    "    docs[tuple(comp_key)] = comp_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 198912/198912 [02:55<00:00, 1135.51it/s]\n"
     ]
    }
   ],
   "source": [
    "vecs = dict()\n",
    "\n",
    "# for each component in this event\n",
    "for urls, tweet_ids in tqdm(docs.items(), total=len(docs)):\n",
    "    vec = []\n",
    "    texts = [event_data[twid].text for twid in tweet_ids]\n",
    "\n",
    "    for tokens in nlp.pipe(texts, n_threads=-1):\n",
    "        for token in tokens:\n",
    "            if not token.like_url and token.lower_ in we:\n",
    "                v = we[token.lower_]\n",
    "                vec.append(v)\n",
    "\n",
    "    if vec:\n",
    "        avg_vec = np.array(vec).mean(axis=0)\n",
    "        vecs[urls] = avg_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = Path('/home/mquezada/CrisisMMD/full_tweets/data/tweet_ids_v1/')\n",
    "vecs_fn = save_path / Path('irma_full_ft_vectors_from_model.pkl')\n",
    "uf_fn = save_path / Path('irma_full_uf_from_model.pkl')\n",
    "event_data_fn = save_path / Path('irma_full_event_data.pkl')\n",
    "\n",
    "\n",
    "with vecs_fn.open('wb') as f:\n",
    "    pkl.dump(vecs, f)\n",
    "    \n",
    "with uf_fn.open('wb') as g:\n",
    "    pkl.dump(uf, g)\n",
    "    \n",
    "with event_data_fn.open('wb') as k:\n",
    "    pkl.dump(event_data, k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "with (save_path / Path('irma_full_ft_vectors_from_model.tsv')).open('w') as f:\n",
    "    for url_key, vec in vecs.items():\n",
    "        key = '\"' + \",,,\".join(url_key) + '\"'\n",
    "        values = \"\\t\".join([str(v) for v in vec])\n",
    "        f.write(f\"{key}\\t{values}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
