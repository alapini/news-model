{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.linalg import norm\n",
    "from numba import jit\n",
    "import random\n",
    "import multiprocessing\n",
    "import re\n",
    "from operator import itemgetter\n",
    "\n",
    "import logging\n",
    "from collections import defaultdict\n",
    "from functools import lru_cache\n",
    "from pathlib import Path\n",
    "from numba import jit\n",
    "import numpy as np\n",
    "\n",
    "import spacy\n",
    "from bson.objectid import ObjectId\n",
    "from gensim.models import KeyedVectors\n",
    "from nltk.corpus import stopwords\n",
    "from pymongo import MongoClient\n",
    "from tqdm import tqdm\n",
    "\n",
    "from load_urls import load_urls\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s : %(message)s', level=logging.INFO)\n",
    "_info = logging.info\n",
    "\n",
    "client = MongoClient('mongodb://localhost:27017')\n",
    "db = client.twitter_news\n",
    "nlp = spacy.load('en_core_web_sm', tagger=False, entity=False, matcher=False)\n",
    "\n",
    "def hashtag_pipe(doc):\n",
    "    merged_hashtag = False\n",
    "    while True:\n",
    "        for token_index, token in enumerate(doc):\n",
    "            if token.text == '#':\n",
    "                if token.head is not None:\n",
    "                    start_index = token.idx\n",
    "                    end_index = start_index + len(token.head.text) + 1\n",
    "                    if doc.merge(start_index, end_index) is not None:\n",
    "                        merged_hashtag = True\n",
    "                        break\n",
    "        if not merged_hashtag:\n",
    "            break\n",
    "        merged_hashtag = False\n",
    "    return doc\n",
    "\n",
    "\n",
    "nlp.add_pipe(hashtag_pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_events = 3\n",
    "\n",
    "\n",
    "@lru_cache(maxsize=total_events)\n",
    "def get_representatives(event_id):\n",
    "    _info(\"getting representatives\")\n",
    "    representatives = db.representatives.find({'event': ObjectId(event_id)})\n",
    "    return list(representatives)\n",
    "\n",
    "\n",
    "@lru_cache(maxsize=total_events)\n",
    "def get_topics(event_id):\n",
    "    _info(\"getting topics\")\n",
    "    topics = list(db.topics.find({'event': ObjectId(event_id)}))\n",
    "    comodin = None\n",
    "    for t in topics:\n",
    "        if t['topic_name'] == \"Non relevant\":\n",
    "            comodin = t\n",
    "            topics.remove(t)\n",
    "            break\n",
    "    return topics, comodin\n",
    "\n",
    "\n",
    "@lru_cache(maxsize=1)\n",
    "def get_events():\n",
    "    _info(\"getting events\")\n",
    "    events = db.events.find()\n",
    "    return list(events)\n",
    "\n",
    "\n",
    "@lru_cache(maxsize=1)\n",
    "def get_tweets(a=None):\n",
    "    _info('getting all tweets')\n",
    "    all_tweets = db.tweets.find()\n",
    "    return list(all_tweets)\n",
    "\n",
    "\n",
    "@lru_cache(maxsize=3)\n",
    "def get_vectors(path):\n",
    "    _info(f\"loading fasttext vectors from {path}\")\n",
    "    word_vectors = KeyedVectors.load_word2vec_format(path)\n",
    "    return word_vectors\n",
    "\n",
    "\n",
    "@lru_cache(maxsize=2**30)\n",
    "def sim(tokens_a, tokens_b):\n",
    "    return ft_comp.n_similarity(tokens_a, tokens_b)\n",
    "\n",
    "\n",
    "def mmr(docs, query, l):\n",
    "    def mmr_score(tweet):\n",
    "        return l * sim(docs[tweet], query) - \\\n",
    "               (1 - l) * max([sim(docs[tweet], docs[y]) for y in set(selected) - {tweet}] or [0])\n",
    "\n",
    "    L = np.array([[l, 0], [0, l - 1]])\n",
    "\n",
    "    def score(tweet):\n",
    "        s1 = sim(docs[tweet], query)\n",
    "        s2 = np.max(np.array([sim(docs[tweet], docs[y]) for y in set(selected) - {tweet}] or [0]))\n",
    "\n",
    "        return L.dot(np.array([s1, s2])).sum()\n",
    "\n",
    "    selected = set()\n",
    "    while selected != set(docs):\n",
    "        remaining = list(set(docs) - selected)\n",
    "        next_selected = max(remaining, key=mmr_score)\n",
    "        # next_selected = remaining[np.argmax([score(t) for t in remaining])]\n",
    "\n",
    "        # next_selected = None\n",
    "        # max_score = 0\n",
    "        #\n",
    "        # for _t in remaining:\n",
    "        #     score = l * sim(docs[_t], query) - \\\n",
    "        #             (1 - l) * max([sim(docs[_t], docs[y]) for y in set(selected) - {_t}] or [0])\n",
    "        #     if score > max_score:\n",
    "        #         max_score = score\n",
    "        #         next_selected = _t\n",
    "\n",
    "        selected.add(next_selected)\n",
    "        yield next_selected, ' '.join(list(docs[next_selected]))\n",
    "\n",
    "\n",
    "@lru_cache(maxsize=total_events)\n",
    "def process_tweets(event_id):\n",
    "    all_tweets = get_tweets()\n",
    "    representatives = get_representatives(event_id)\n",
    "\n",
    "    _info(\"processing tweets\")\n",
    "\n",
    "    # rep_tweet: repr_id => tweet\n",
    "    rep_tweet = dict()\n",
    "    for t in tqdm(all_tweets):\n",
    "        rep_tweet[t['representative']] = t\n",
    "\n",
    "    # repr_ids: {repr_id} // this event\n",
    "    repr_ids = set([r['_id'] for r in representatives])\n",
    "\n",
    "    # tweets_this_event: [tweet]\n",
    "    tweets_this_event = [t for r, t in rep_tweet.items() if r in repr_ids]\n",
    "\n",
    "    tweets_tokens = dict()\n",
    "    all_tokens = set()\n",
    "    for tweet, doc in tqdm(zip(tweets_this_event, nlp.pipe([_t['text'] for _t in tweets_this_event],\n",
    "                                                           n_threads=8,\n",
    "                                                           batch_size=1024)),\n",
    "                           total=len(tweets_this_event)):\n",
    "\n",
    "        tokens = frozenset([token.lower_\n",
    "                            for token in doc\n",
    "                            if token.lower_ not in stopwords.words('english') and token.lower_ in ft_comp])\n",
    "\n",
    "        if tokens and tokens not in all_tokens:\n",
    "            tweets_tokens[str(tweet['_id'])] = tokens\n",
    "            all_tokens.add(tokens)\n",
    "\n",
    "    return tweets_tokens\n",
    "\n",
    "\n",
    "def expand_query(topics):\n",
    "    # query expansion \n",
    "    topics_this_event = []\n",
    "    for topic in topics:\n",
    "        new_words = set()\n",
    "        for keyword in topic:\n",
    "            new_words |= set([word for word, _ in ft_comp.most_similar(keyword, topn=25)])\n",
    "        new_topic = set(topic) | new_words\n",
    "        topics_this_event.append(new_topic)\n",
    "    return topics_this_event"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = Path('/home/mquezada/tweet_topics/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-07-06 16:05:17,529 : getting all tweets\n"
     ]
    }
   ],
   "source": [
    "all_tweets = get_tweets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'_id': ObjectId('5b17173ada870923dcb1c674'),\n",
       " 'tweet_id': 445478488016171008,\n",
       " 'text': 'The Oscar Pistorius murder trial - the little-reported legal questions - Legalweek http://t.co/VlhpZigPBR',\n",
       " 'created_at': datetime.datetime(2014, 3, 17, 8, 35, 22),\n",
       " 'retweet_id': None,\n",
       " 'reply_id': None,\n",
       " 'short_urls': ['http://t.co/VlhpZigPBR'],\n",
       " 'expanded_urls': [None],\n",
       " 'representative': ObjectId('5b17173ada870923dcb1c673')}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = all_tweets[random.randint(0, len(all_tweets))]\n",
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 642251/642251 [00:00<00:00, 1285566.79it/s]\n"
     ]
    }
   ],
   "source": [
    "filtered_tweets = []\n",
    "for t in tqdm(all_tweets):\n",
    "    e_u = t['expanded_urls']\n",
    "    if all(u is not None for u in e_u):\n",
    "        filtered_tweets.append(t)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "557622"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(filtered_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
