{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymongo import MongoClient\n",
    "from bson.objectid import ObjectId\n",
    "from collections import OrderedDict\n",
    "import random\n",
    "import logging\n",
    "from datetime import datetime\n",
    "from functools import lru_cache\n",
    "from paths import data_path\n",
    "from pathlib import Path\n",
    "from gensim.models import KeyedVectors\n",
    "from tqdm import tqdm\n",
    "from pprint import pprint\n",
    "import spacy\n",
    "from nltk.corpus import stopwords\n",
    "import pyemd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from sklearn.cluster import KMeans, AgglomerativeClustering\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(format='%(asctime)s : %(message)s', level=logging.INFO)\n",
    "_info = logging.info\n",
    "\n",
    "client = MongoClient('mongodb://localhost:27017')\n",
    "db = client.twitter_news\n",
    "\n",
    "counters = dict()\n",
    "\n",
    "total_events = 3\n",
    "\n",
    "@lru_cache(maxsize=total_events)\n",
    "def get_representatives(event_id):\n",
    "    _info(\"getting representatives\")\n",
    "    representatives = db.representatives.find({'event': ObjectId(event_id)})\n",
    "    return list(representatives)\n",
    "\n",
    "\n",
    "@lru_cache(maxsize=total_events)\n",
    "def get_topics(event_id):\n",
    "    _info(\"getting topics\")\n",
    "    topics = list(db.topics.find({'event': ObjectId(event_id)}))\n",
    "    for t in topics:\n",
    "        if t['topic_name'] == \"Non relevant\":\n",
    "            comodin = t\n",
    "            topics.remove(t)\n",
    "            break\n",
    "    return topics, comodin\n",
    "\n",
    "\n",
    "@lru_cache(maxsize=1)\n",
    "def get_events():\n",
    "    _info(\"getting events\")\n",
    "    events = db.events.find()\n",
    "    return list(events)\n",
    "\n",
    "\n",
    "@lru_cache(maxsize=1)\n",
    "def get_tweets(a=None):\n",
    "    _info('getting all tweets')\n",
    "    all_tweets = db.tweets.find()\n",
    "    return list(all_tweets)\n",
    "\n",
    "\n",
    "@lru_cache(maxsize=3)\n",
    "def get_vectors(path):\n",
    "    _info(f\"loading fasttext vectors from {path}\")\n",
    "    word_vectors = KeyedVectors.load_word2vec_format(path)\n",
    "    return word_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm', tagger=False, entity=False, matcher=False)\n",
    "\n",
    "def hashtag_pipe(doc):\n",
    "    merged_hashtag = False\n",
    "    while True:\n",
    "        for token_index, token in enumerate(doc):\n",
    "            if token.text == '#':\n",
    "                if token.head is not None:\n",
    "                    start_index = token.idx\n",
    "                    end_index = start_index + len(token.head.text) + 1\n",
    "                    if doc.merge(start_index, end_index) is not None:\n",
    "                        merged_hashtag = True\n",
    "                        break\n",
    "        if not merged_hashtag:\n",
    "            break\n",
    "        merged_hashtag = False\n",
    "    return doc\n",
    "\n",
    "nlp.add_pipe(hashtag_pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-06-20 15:46:20,163 : loading fasttext vectors from /home/mquezada/anchor-text-twitter/data/ft_alltweets_model.vec\n",
      "2018-06-20 15:46:20,164 : loading projection weights from /home/mquezada/anchor-text-twitter/data/ft_alltweets_model.vec\n",
      "2018-06-20 15:47:22,853 : loaded (1076139, 100) matrix from /home/mquezada/anchor-text-twitter/data/ft_alltweets_model.vec\n",
      "2018-06-20 15:47:22,854 : loading fasttext vectors from /home/mquezada/anchor-text-twitter/data/ft_all_tweets_1line1tweet.vec\n",
      "2018-06-20 15:47:22,854 : loading projection weights from /home/mquezada/anchor-text-twitter/data/ft_all_tweets_1line1tweet.vec\n",
      "2018-06-20 15:48:29,089 : loaded (1157600, 100) matrix from /home/mquezada/anchor-text-twitter/data/ft_all_tweets_1line1tweet.vec\n",
      "2018-06-20 15:48:29,090 : loading fasttext vectors from /home/mquezada/anchor-text-twitter/data/w2v_all_tweets.txt\n",
      "2018-06-20 15:48:29,090 : loading projection weights from /home/mquezada/anchor-text-twitter/data/w2v_all_tweets.txt\n",
      "2018-06-20 15:51:27,558 : loaded (1157599, 300) matrix from /home/mquezada/anchor-text-twitter/data/w2v_all_tweets.txt\n",
      "2018-06-20 15:51:27,559 : loading fasttext vectors from /home/mquezada/anchor-text-twitter/data/w2v_all_tweets_1_line_1_component.txt\n",
      "2018-06-20 15:51:27,560 : loading projection weights from /home/mquezada/anchor-text-twitter/data/w2v_all_tweets_1_line_1_component.txt\n",
      "2018-06-20 15:53:20,502 : loaded (1076140, 200) matrix from /home/mquezada/anchor-text-twitter/data/w2v_all_tweets_1_line_1_component.txt\n"
     ]
    }
   ],
   "source": [
    "path = data_path / Path('/home/mquezada/anchor-text-twitter/data/ft_alltweets_model.vec')\n",
    "ft_comp = get_vectors(path.as_posix())\n",
    "\n",
    "path = data_path / Path('/home/mquezada/anchor-text-twitter/data/ft_all_tweets_1line1tweet.vec')\n",
    "ft_all = get_vectors(path.as_posix())\n",
    "\n",
    "path = data_path / Path('/home/mquezada/anchor-text-twitter/data/w2v_all_tweets.txt')\n",
    "w2v_all = get_vectors(path.as_posix())\n",
    "\n",
    "path = data_path / Path('/home/mquezada/anchor-text-twitter/data/w2v_all_tweets_1_line_1_component.txt')\n",
    "w2v_comp = get_vectors(path.as_posix())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ft_comp\n",
      "0.33911799442295903\n",
      "\n",
      "ft_all\n",
      "0.3300593981594927\n",
      "\n",
      "w2v_comp\n",
      "0.09744087254883523\n",
      "\n",
      "w2v_all\n",
      "0.1679580041090767\n"
     ]
    }
   ],
   "source": [
    "w = 'iphone'\n",
    "w2 = 'earthquake'\n",
    "\n",
    "print(\"ft_comp\")\n",
    "pprint(ft_comp.similarity(w, w2))\n",
    "print()\n",
    "print(\"ft_all\")\n",
    "pprint(ft_all.similarity(w, w2))\n",
    "print()\n",
    "print(\"w2v_comp\")\n",
    "pprint(w2v_comp.similarity(w, w2))\n",
    "print()\n",
    "print(\"w2v_all\")\n",
    "pprint(w2v_all.similarity(w, w2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Obtener los tweets del evento y crear mapa de representantes-tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-06-20 16:47:51,169 : getting all tweets\n",
      "2018-06-20 16:47:57,164 : getting events\n",
      "2018-06-20 16:47:57,165 : getting representatives\n",
      "2018-06-20 16:47:57,269 : getting topics\n",
      "100%|██████████| 642251/642251 [00:00<00:00, 1273338.48it/s]\n"
     ]
    }
   ],
   "source": [
    "all_tweets = get_tweets()\n",
    "events = get_events()\n",
    "event = events[0]\n",
    "\n",
    "representatives = get_representatives(event['_id'])\n",
    "topics, _ = get_topics(event['_id'])\n",
    "\n",
    "rep_tweet = dict()\n",
    "for t in tqdm(all_tweets):\n",
    "    rep_tweet[t['representative']] = t\n",
    "\n",
    "rep_set = set([r['_id'] for r in representatives])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14233"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# asociacion entre representativos y tweets\n",
    "\n",
    "tweets_this_event = [t for r, t in rep_tweet.items() if r in rep_set]\n",
    "len(tweets_this_event)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Listar topicos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Car bomb explodes\n",
      "ISIS adjudicates attack\n",
      "Report on the amount of casualties\n",
      "Hostages are taken\n",
      "Report on the number of attackers\n",
      "Confrontation with security forces\n"
     ]
    }
   ],
   "source": [
    "for topic in topics:\n",
    "    print(topic['topic_name'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generar tokens por cada tweet en `tweet_tokens`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14233/14233 [00:46<00:00, 308.91it/s]\n"
     ]
    }
   ],
   "source": [
    "# gen tokens for each tweet in the event\n",
    "\n",
    "tweets_tokens = set()\n",
    "for doc in tqdm(nlp.pipe(map(lambda t: t['text'], tweets_this_event), n_threads=8, batch_size=1024), \n",
    "                total=len(tweets_this_event)):\n",
    "    \n",
    "    tokens = frozenset([token.lower_ \n",
    "                        for token in doc \n",
    "                        if token.lower_ not in stopwords.words('english') and token.lower_ in vectors])\n",
    "    \n",
    "    if tokens: \n",
    "        tweets_tokens.add(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sim1(tokens_a, tokens_b):\n",
    "    ta = [t for t in tokens_a if t in ft_all]\n",
    "    tb = [t for t in tokens_b if t in ft_all]\n",
    "    return ft_all.n_similarity(ta, tb)\n",
    "\n",
    "def sim2(tokens_a, tokens_b):\n",
    "    ta = [t for t in tokens_a if t in ft_comp]\n",
    "    tb = [t for t in tokens_b if t in ft_comp]\n",
    "    return ft_comp.n_similarity(ta, tb)\n",
    "\n",
    "def sim3(tokens_a, tokens_b):\n",
    "    ta = [t for t in tokens_a if t in w2v_all]\n",
    "    tb = [t for t in tokens_b if t in w2v_all]\n",
    "    return w2v_all.n_similarity(ta, tb)\n",
    "\n",
    "def sim4(tokens_a, tokens_b):\n",
    "    ta = [t for t in tokens_a if t in w2v_comp]\n",
    "    tb = [t for t in tokens_b if t in w2v_comp]\n",
    "    return w2v_comp.n_similarity(ta, tb)\n",
    "\n",
    "def mmr(docs, q, lambda_, sim):\n",
    "    selected = OrderedDict()\n",
    "    while set(selected) != docs:\n",
    "        remaining = docs - set(selected)\n",
    "        mmr_score = lambda x: lambda_ * sim(x, q) - (1 - lambda_) * max([sim(x, y) for y in set(selected) - {x}] or [0])\n",
    "        next_selected = argmax(remaining, mmr_score)\n",
    "        selected[next_selected] = len(selected)\n",
    "        yield selected   \n",
    "\n",
    "def argmax(keys, f):\n",
    "    return max(keys, key=f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'car', 'bomb', 'explodes'}\n",
      "\n",
      "ft_all\n",
      "park corinthia car bomb explodes\n",
      "woooooah\n",
      "via ignorant shit\n",
      "#cars luxury sleep $ 85 tesla model\n",
      "3 kills hotel car bomb outside\n",
      "hotel tripoli car dlvr bomb outside explodes\n",
      "#tripoli middle east hotel car bomb outside explodes\n",
      "libya isis car claims bomb\n",
      "libya witnesses hotel car bomb outside explodes capital\n",
      "hotel tripoli car bomb outside explodes\n",
      "\n",
      "ft_comp\n",
      "park corinthia car bomb explodes\n",
      "libya witnesses hotel car bomb outside explodes capital\n",
      "#cars luxury sleep $ 85 tesla model\n",
      "via ignorant shit\n",
      "post hotel new tripoli car bomb outside explodes\n",
      "hotel tripoli car dlvr bomb outside explodes\n",
      "#tripoli middle east hotel car bomb outside explodes\n",
      "hotel tripoli car bomb outside explodes\n",
      "3 kills hotel car bomb outside\n",
      "libya isis car claims bomb\n",
      "\n",
      "w2v_all\n",
      "park corinthia car bomb explodes\n",
      "rt inspiration\n",
      "libya isis car claims bomb\n",
      "libya witnesses hotel car bomb outside explodes capital\n",
      "upgrade limited suite sweet moment deal package\n",
      "building weirdest ever\n",
      "newsworld via #popularworldnews hotel world tripoli car bomb explodes\n",
      "hotel #news latest tripoli car bomb outside explodes\n",
      "#tripoli middle east hotel car bomb outside explodes\n",
      "break confirmed hotel shooting ppl began car talkin bomb\n",
      "\n",
      "w2v_comp\n",
      "park corinthia car bomb explodes\n",
      "via ignorant shit\n",
      "libya isis car claims bomb\n",
      "newsworld via #popularworldnews hotel world tripoli car bomb explodes\n",
      "breaking\n",
      "gun hotel attack lybian amp bomb\n",
      "break confirmed hotel shooting ppl began car talkin bomb\n",
      "car driver hotel az man tesla uber offers\n",
      "3 kills hotel car bomb outside\n",
      "poor resorts gov't moves phillipines secretly arrives homeless\n",
      "\n",
      "\n",
      "{'adjudicates', 'isis', 'attack'}\n",
      "\n",
      "ft_all\n",
      "1 libyan | hotel attack american degrade isis destroy terror launches much killed brazen\n",
      "amit dear rates check tariff\n",
      "abu attacked name operative anas intelligence nato al libi\n",
      "peace religion\n",
      "vide con inter laico worries kempinski\n",
      "hotel attack isis tripoli claims responsibility\n",
      "recruiting\n",
      "jackie sparrow\n",
      "islam says och assault libyan behind hotel isis igen linked\n",
      "barack thanks hussein\n",
      "\n",
      "ft_comp\n",
      "libyan says assault terrorist significant behind hotel attack isis deadly string linked group\n",
      "#isis #tripoli claiming hotel bombing corinthia responsibility\n",
      "peace religion\n",
      "extremists islamic libyan claimed ca hotel attack corinthia responsibility state\n",
      "libya isis car claims bomb\n",
      "amit dear rates check tariff\n",
      "united libya libyan mission hotel overruns genocide isis committing accomplished nations used\n",
      "abu attacked name libyan operative luxury anas hotel #tlot tcot intelligence al libi infowars\n",
      "gun hotel attack lybian amp bomb\n",
      "hostages libyan terrorist news attack taken\n",
      "\n",
      "w2v_all\n",
      "libya 3 islamic assault fighters claimed group hotel attack isis tripoli killed claims state\n",
      "jackie sparrow\n",
      "someone #valentinesday special spoiling\n",
      "rt\n",
      "midnight quick,#win family overnight closes stay\n",
      "prime lyb attack minister main\n",
      "recruiting\n",
      "peace religion\n",
      "authenticity stuffy rt luxury hotel gilded millennial design customers love hate\n",
      "obs war\n",
      "\n",
      "w2v_comp\n",
      "libya 's hotel attack isis terror today claims responsibility\n",
      "via ignorant shit\n",
      "recruiting\n",
      "obs war\n",
      "#isis least #tripoli militants storm 8 killed\n",
      "libya away setting 's picking board isis groups europe collaborating spring infrastructure\n",
      "case another workplace isolated violence\n",
      "abu attacked name operative anas intelligence nato al libi\n",
      "islamic says affiliate assault libyan behind attack peop many five killed state\n",
      "spotted anyone #luxuryukhotel us todays\n",
      "\n",
      "\n",
      "{'on', 'report', 'of', 'amount', 'casualties', 'the'}\n",
      "\n",
      "ft_all\n",
      "opens 1 breaking monitoring slooow news day story fire tourist hotel noted today earlier\n",
      "enough brave\n",
      "menace going global\n",
      "barack thanks hussein\n",
      "shall attentive ask extremely attention luxury detail travel amp due receive world love staff\n",
      "abu attacked name operative anas intelligence al libi\n",
      "peace religion\n",
      "case another workplace isolated violence\n",
      "success lol another story yet #obama #libya\n",
      "'s\n",
      "\n",
      "ft_comp\n",
      "opens 1 breaking monitoring slooow news day story fire tourist hotel noted today earlier\n",
      "enough brave\n",
      "menace going global\n",
      "establish opportunity luxury internal hotel auditor great programme risk audit experienced based\n",
      "hv #superbowltix news going mt ur sold faith\n",
      "guests aside staying rude obnoxious largely luxury cost hotel turns amp downside\n",
      "peace religion\n",
      "places beautiful grange many n't sands earth\n",
      "united libya libyan mission hotel overruns genocide isis committing accomplished nations used\n",
      "case another workplace isolated violence\n",
      "\n",
      "w2v_all\n",
      "near info lots #tripoli blast morning say hear 's hotel casualties corinthia sources sirens\n",
      "'s\n",
      "life card trump limits 'll boundaries compromises without member experience\n",
      "#world would amount #hotelrooms spend --&gt money one night\n",
      "continues resorts fairmont expansion amp #hotelnews china\n",
      "b&amp;b says best yorkshire world\n",
      "breaking\n",
      "via levels ridiculous 's hotel designs new china reach\n",
      "pleasure 's two reading en today route stories\n",
      "mo rt see 's thanks saying world national geographic\n",
      "\n",
      "w2v_comp\n",
      "gunfight breaking #tripoli hotel casualties details corinthia still yet happening #libya\n",
      "#world would amount #hotelrooms spend --&gt money one night\n",
      "via levels ridiculous 's hotel designs new china reach\n",
      "prime lyb attack minister main\n",
      "spotted anyone #luxuryukhotel us todays\n",
      "case another workplace isolated violence\n",
      "telsa #property van man kipped following listing alternative\n",
      "'s\n",
      "nbc motel via | residents say 10 captives troubled feel philly makes like homes\n",
      "pleasure 's two reading en today route stories\n",
      "\n",
      "\n",
      "{'hostages', 'taken', 'are'}\n",
      "\n",
      "ft_all\n",
      "3 hostages dead tripoli taken update\n",
      "stepping instead corporate without sort great decor place making like feel style\n",
      "norton safe identity\n",
      "motel residents outraged troubled reopening\n",
      "recruiting\n",
      "hostages lets attention deflating balls patriots hotel pay tripoli sure even taken\n",
      "hostages witnesses say #amn hotel started details next randomly firing staff taken men gunmen #libya\n",
      "claim libya hostages islamic hotel attack taken state\n",
      "muslims hostages #tripoli hotel peace loving soon corinthia hope everyone taken rescued #libya\n",
      "motel residents say captives troubled feel philly makes like\n",
      "\n",
      "ft_comp\n",
      "hostages libyan posted forces video websites th surround tripoli hotel;12 taken group update\n",
      "enough brave\n",
      "visited last family year one 're\n",
      "hostages lets attention deflating balls patriots hotel pay tripoli sure even taken\n",
      "norton safe identity\n",
      "family arrive finally luxury hotel trans\n",
      "barack thanks hussein\n",
      "peace religion\n",
      "motel residents outraged troubled reopening\n",
      "muslims hostages #tripoli hotel peace loving soon corinthia hope everyone taken rescued #libya\n",
      "\n",
      "w2v_all\n",
      "3 hostages dead tripoli taken update\n",
      "events look team course take thanks contact feel free please\n",
      "#architecture soma constructing innovation luxury architects doha\n",
      "motel residents outraged troubled reopening\n",
      "coming #kabari\n",
      "questions features rooms beds two hotel charleston answered queen recommend\n",
      "hostages lets attention deflating balls patriots hotel pay tripoli sure even taken\n",
      "claim libya hostages islamic hotel attack taken state\n",
      "hostages witnesses say #amn hotel started details next randomly firing staff taken men gunmen #libya\n",
      "luxury taken hotel trans\n",
      "\n",
      "w2v_comp\n",
      "hostages hotel corinthia taken #libya\n",
      "gone believe #staycalm fools dood\n",
      "motel residents say captives troubled feel philly makes like\n",
      "spotted anyone #luxuryukhotel us todays\n",
      "hostages lets attention deflating balls patriots hotel pay tripoli sure even taken\n",
      "woooooah\n",
      "terrorist looks impossible business truth operate killed\n",
      "reviewing products one luxe\n",
      "kill least hostages tripol well rt working 8 take arab spring gunmen\n",
      "breaking\n",
      "\n",
      "\n",
      "{'on', 'number', 'report', 'of', 'the', 'attackers'}\n",
      "\n",
      "ft_all\n",
      "3 report tripoli hotel make corinthia surrounded security n't states pages sense directorate suspects\n",
      "thanks post latest\n",
      "visited last family year one 're\n",
      "breaking\n",
      "finest 's africa become kenya\n",
      "website online directly look already book us booked take n't also\n",
      "case another workplace isolated violence\n",
      "purse 's people world friendly rated hotels\n",
      "opens 1 breaking monitoring slooow news day story fire tourist hotel noted today earlier\n",
      "pleasure 's two reading en today route stories\n",
      "\n",
      "ft_comp\n",
      "opens 1 breaking monitoring slooow news day story fire tourist hotel noted today earlier\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "barack thanks hussein\n",
      "amit dear rates check tariff\n",
      "abu attacked name operative anas intelligence al libi\n",
      "enough brave\n",
      "menace going global\n",
      "peace religion\n",
      "blue read tile perfection signature find quest extraordinary teal\n",
      "rt\n",
      "profile company community created owler\n",
      "\n",
      "w2v_all\n",
      "reporting #tripoli rt 's news hotel local important multiple corinthia also gunmen hostage\n",
      "'s\n",
      "visited last family year one 're\n",
      "essential trump card 's tell comfort us sign\n",
      "purse 's people world friendly rated hotels\n",
      "near story posts project huge opening may renovation 2015\n",
      "baccarat finally 's hotel going open york new\n",
      "via #hoteldeparis monaco 's pictures\n",
      "pleasure 's two reading en today route stories\n",
      "named number diamond aaa luxury hotel us ranking amp retaining 5 2 congrats\n",
      "\n",
      "w2v_comp\n",
      "27/1/15 reporting number confirmed figures news hotel held important corinthia time al gunmen naba\n",
      "'s\n",
      "case another workplace isolated violence\n",
      "near cross bed house glazed luxury 6 detached home large #travel cuffley holiday double w waltham\n",
      "prime lyb attack minister main\n",
      "twitter read luxury --&gt uk tweets must amp hotels photographs b&amp;bs\n",
      "3 injured #tripoli attk 6 hotel 2 inside dealing ppl still security amp killed attackers\n",
      "spotted anyone #luxuryukhotel us todays\n",
      "breaking\n",
      "pm #tripoli seen attack link corinthia malta yet\n",
      "\n",
      "\n",
      "{'forces', 'with', 'confrontation', 'security'}\n",
      "\n",
      "ft_all\n",
      "#tripoli libyan storming forces crisis capable hotel geared security resolving 5 combat reports gunmen #libya\n",
      "rt\n",
      "upgrade limited suite sweet moment deal package\n",
      "peace religion\n",
      "case another workplace isolated violence\n",
      "motel residents say captives troubled feel philly makes like\n",
      "barack thanks hussein\n",
      "fatal libyan report morning received threat hotel attack night world bomb deadly tuesday\n",
      "events mpi vital time meetings contingency plans\n",
      "obs war\n",
      "\n",
      "ft_comp\n",
      "libyan stand ten forces dead two hotel dw_english attack hours assaila security leaves\n",
      "trump card repeating request member need 're\n",
      "case another workplace isolated violence\n",
      "poor resorts gov't moves phillipines secretly arrives homeless\n",
      "norton safe identity\n",
      "recruiting\n",
      "united libyan lied mission hotel overruns remember isis time obama accomplished nations used\n",
      "vacuum change hotel ongoing hostage regime leaving situation site power abhors\n",
      "breaking\n",
      "militias official assessed ability army ruling situation limited corinthia hotel- tripoli deal hostage\n",
      "\n",
      "w2v_all\n",
      "libya 3 kills forces crisis guards security surround tripoli car bomb hostage\n",
      "someone #valentinesday special spoiling\n",
      "poor resorts gov't moves phillipines secretly arrives homeless\n",
      "rt\n",
      "obs war\n",
      "life card trump limits 'll boundaries compromises without member experience\n",
      "coming #kabari\n",
      "norton safe identity\n",
      "case another workplace isolated violence\n",
      "near story posts project huge opening may renovation 2015\n",
      "\n",
      "w2v_comp\n",
      "#tripoli libyan storming forces crisis capable hotel geared security resolving 5 combat reports gunmen #libya\n",
      "life card trump limits 'll boundaries compromises without member experience\n",
      "case another workplace isolated violence\n",
      "shot would luxury hotel drug face linens deal\n",
      "barack thanks hussein\n",
      "obs war\n",
      "poor resorts gov't moves phillipines secretly arrives homeless\n",
      "events mpi vital time meetings contingency plans\n",
      "prime lyb attack minister main\n",
      "7603 pickup vippartybuses call service rt luxury 732 877 airport us hotel- reserve today direct\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lambda_ = 0.5\n",
    "\n",
    "for topic in topics:\n",
    "    query = set(map(lambda s: s.lower(), topic['topic_name'].split()))\n",
    "\n",
    "    print(query)\n",
    "    print()\n",
    "    \n",
    "    i = 0\n",
    "    print(\"ft_all\")\n",
    "    for doc in mmr(tweets_tokens, query, lambda_, sim1):\n",
    "        print(' '.join(list(doc.items())[-1][0]))\n",
    "        i += 1\n",
    "        if i == 10:\n",
    "            break\n",
    "\n",
    "    i = 0\n",
    "    print(\"\\nft_comp\")\n",
    "    for doc in mmr(tweets_tokens, query, lambda_, sim2):\n",
    "        print(' '.join(list(doc.items())[-1][0]))\n",
    "        i += 1\n",
    "        if i == 10:\n",
    "            break\n",
    "\n",
    "    i = 0\n",
    "    print(\"\\nw2v_all\")\n",
    "    for doc in mmr(tweets_tokens, query, lambda_, sim3):\n",
    "        print(' '.join(list(doc.items())[-1][0]))\n",
    "        i += 1\n",
    "        if i == 10:\n",
    "            break\n",
    "\n",
    "    i = 0\n",
    "    print(\"\\nw2v_comp\")\n",
    "    for doc in mmr(tweets_tokens, query, lambda_, sim4):\n",
    "        print(' '.join(list(doc.items())[-1][0]))\n",
    "        i += 1\n",
    "        if i == 10:\n",
    "            break\n",
    "    \n",
    "    print()\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(frozenset({'#kabari', 'coming'}), 9)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(doc.items())[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El problema con lo siguiente son los topicos: las palabras definen los tweets que se van a obtener con la búsqueda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['gunmen', 'storm', 'luxury', 'hotel', 'libya', 'capital', 'least', '3', 'killed', 'hotel', 'popular', 'foreigners', 'c', '#muhamadjabal']\n",
      "\n",
      "['car', 'bomb', 'explodes']\n",
      "0.6755222282719202\n",
      "\n",
      "['isis', 'adjudicates', 'attack']\n",
      "0.6617886776347398\n",
      "\n",
      "['report', 'amount', 'casualties']\n",
      "0.6782251905755576\n",
      "\n",
      "['hostages', 'taken']\n",
      "0.7047494552877931\n",
      "\n",
      "['report', 'number', 'attackers']\n",
      "0.7330240186949215\n",
      "\n",
      "['confrontation', 'security', 'forces']\n",
      "0.6800127367584079\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# convertir topicos en word vectors\n",
    "# convertir tweets del evento (representativos) en vectores\n",
    "## app lowercase\n",
    "## app tokenize\n",
    "## del urls\n",
    "# computar similitud entre topico y tweet\n",
    "\n",
    "rnd_tweet = random.choice(tweets_this_event)\n",
    "text = nlp(rnd_tweet['text'])\n",
    "\n",
    "tweet_tokens = []\n",
    "\n",
    "for token in text:\n",
    "    if token.lower_ in stopwords.words('english') or token.lower_ not in vectors:\n",
    "        continue\n",
    "    tweet_tokens.append(token.lower_)\n",
    "\n",
    "topic_tokens = []\n",
    "for topic in nlp.pipe([t['topic_name'] for t in topics]):\n",
    "    topic_tokens.append([token.lower_ for token in topic \n",
    "                         if token.lower_ in vectors and token.lower_ not in stopwords.words('english')])\n",
    "    \n",
    "print(tweet_tokens)\n",
    "print()\n",
    "for tokens in topic_tokens:\n",
    "    print(tokens)\n",
    "    print(vectors.n_similarity(tweet_tokens, tokens))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Idea: hacer clustering de palabras del vocabulario y samplear tweets a partir de los clusters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14233/14233 [01:48<00:00, 130.97it/s]\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(preprocessor=lambda x: x, tokenizer=lambda x: x)\n",
    "\n",
    "m = vectorizer.fit_transform(tweets_tokens)\n",
    "m2 = m.transpose()  # represent words instead of documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(3, 7784), (6, 6), (4, 5), (13, 5), (12, 5), (14, 5), (1, 5), (2, 4), (17, 4), (15, 4), (19, 3), (18, 2), (16, 2), (0, 2), (9, 1), (8, 1), (11, 1), (7, 1), (10, 1), (5, 1)]\n",
      "\n",
      "['pic', 'trans']\n",
      "\n",
      "['kill', 'killed', 'news', 'says', 'security']\n",
      "\n",
      "['3', 'guards', 'hostages', 'take']\n",
      "\n",
      "[\"'s\", 'bomb', 'car', 'explodes', 'outside']\n",
      "\n",
      "['tripoli']\n",
      "\n",
      "['foreigner', 'linked', 'militant', 'popular', 'possibly', 'top']\n",
      "\n",
      "['gunmen']\n",
      "\n",
      "['capital']\n",
      "\n",
      "['attack']\n",
      "\n",
      "['libyan']\n",
      "\n",
      "['design']\n",
      "\n",
      "['corinthia', 'luxurious', 'shot', 'tuesday', 'way']\n",
      "\n",
      "['affiliate', 'assault', 'behind', 'islamic', 'state']\n",
      "\n",
      "['dead', 'eight', 'least', 'official', 'storm']\n",
      "\n",
      "['exclusive', 'offers', 'special', 'visa']\n",
      "\n",
      "['group', 'isis']\n",
      "\n",
      "['businessweek', 'ei', 'killing', 'militants']\n",
      "\n",
      "['5', 'foreigners']\n",
      "\n",
      "['hotel', 'libya', 'luxury']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "all_words = np.array(vectorizer.get_feature_names())\n",
    "\n",
    "nc = 20\n",
    "km = KMeans(n_clusters=nc, n_jobs=-1, n_init=30, max_iter=500)\n",
    "km.fit(m2)\n",
    "\n",
    "print(Counter(km.labels_).most_common(nc))\n",
    "print()\n",
    "\n",
    "for i in range(nc):\n",
    "    words = all_words[km.labels_ == i].tolist()\n",
    "    if len(words) < 20:\n",
    "        pprint(words)\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "agglomerative clustering doesn't work well in this case:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AgglomerativeClustering(affinity='cosine', compute_full_tree='auto',\n",
       "            connectivity=None, linkage='complete', memory='/tmp',\n",
       "            n_clusters=20, pooling_func=<function mean at 0x7f6711ed8ae8>)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agg = AgglomerativeClustering(n_clusters=20, affinity=\"cosine\", memory=\"/tmp\", linkage=\"complete\")\n",
    "\n",
    "agg.fit(m2.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['#vofnafrica', 'dictator', 'moam', 'ouster', '|']\n",
      "\n",
      "['gulbenkian', 'km', 'lisbon', 'located', 'mi', 'rios', 'sete', 'within', 'zoo']\n",
      "\n",
      "['#yugvani', 'claim', 'zee']\n",
      "\n",
      "['appears', 'reveal', '~10:00']\n",
      "\n",
      "['art', 'dolder', 'incomparable', 'zurich']\n",
      "\n",
      "['african', 'news24', '|via']\n",
      "\n",
      "['blaze', '~via']\n",
      "\n",
      "['#architecture', 'zermatt']\n",
      "\n",
      "['girls', 'menara', 'ze']\n",
      "\n",
      "['zzzzzz']\n",
      "\n",
      "['~inhabitat']\n",
      "\n",
      "['ali', 'd.', 'david', 'kirkpatrick', 'suliman', 'zway']\n",
      "\n",
      "['bbcnews', '|thedailypr']\n",
      "\n",
      "['#eventsus', '#ticket', '12,500', 'nfl', 'xlix', 'zone']\n",
      "\n",
      "['bachelor', 'gadgets', '|the']\n",
      "\n",
      "['happens', 'zokmed']\n",
      "\n",
      "['baca', 'lt;-', 'selengkapnya', '|news|=']\n",
      "\n",
      "['zealand']\n",
      "\n",
      "['||']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(20):\n",
    "    words = all_words[agg.labels_ == i].tolist()\n",
    "    if len(words) < 50:\n",
    "        pprint(words)\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-06-15 12:32:58,021 : precomputing L2-norms of word weight vectors\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('saintiago', 0.8565769791603088),\n",
       " ('#santiago', 0.841245174407959),\n",
       " ('santia', 0.8093695640563965),\n",
       " ('nacional', 0.7871439456939697),\n",
       " ('#ripsantiago', 0.7835432291030884),\n",
       " ('cristobal', 0.7817075252532959),\n",
       " ('chilea', 0.7602672576904297),\n",
       " ('santo', 0.7583289742469788),\n",
       " ('pestanosantiago', 0.7576349377632141),\n",
       " ('santiam', 0.7572433948516846),\n",
       " ('chilean', 0.7534645795822144),\n",
       " ('chileans', 0.7500560283660889),\n",
       " ('vicente', 0.7482686042785645),\n",
       " ('neuquen', 0.7467107176780701),\n",
       " ('pacheco', 0.7450253963470459),\n",
       " ('centro', 0.7428617477416992),\n",
       " ('penitencia', 0.7422252297401428),\n",
       " ('#antoniosantiago', 0.7407231330871582),\n",
       " ('monterrey', 0.7407140731811523),\n",
       " ('#erupcionvillarrica', 0.7407123446464539),\n",
       " ('agencia', 0.7366681694984436),\n",
       " ('#santiagobernabeu', 0.7362847328186035),\n",
       " ('libertadores', 0.7348600625991821),\n",
       " ('camino', 0.733985185623169),\n",
       " ('santos', 0.7329348921775818),\n",
       " ('ensenada', 0.732906699180603),\n",
       " ('chileing', 0.7327314615249634),\n",
       " ('strano', 0.7322192192077637),\n",
       " ('caraballo', 0.7308638691902161),\n",
       " ('reddeemergencia', 0.7296812534332275),\n",
       " ('davao', 0.7268149852752686),\n",
       " ('araucania', 0.7258568406105042),\n",
       " ('jorge', 0.7258443832397461),\n",
       " ('chile#spain', 0.7223672866821289),\n",
       " ('villarre', 0.721317708492279),\n",
       " ('perudate', 0.7212801575660706),\n",
       " ('cortes', 0.7210659980773926),\n",
       " ('santino', 0.7197098135948181),\n",
       " ('canteras', 0.7191683053970337),\n",
       " ('#santiagodecuba', 0.7185970544815063),\n",
       " ('montesion', 0.7182221412658691),\n",
       " ('stratovolcano', 0.717942476272583),\n",
       " ('aguado', 0.7172447443008423),\n",
       " ('guatema', 0.7169990539550781),\n",
       " ('iaquinta', 0.7169511318206787),\n",
       " ('pasillo', 0.7166237235069275),\n",
       " ('villarruel', 0.7165982127189636),\n",
       " ('laquinta', 0.7159038782119751),\n",
       " ('campo', 0.7154406309127808),\n",
       " ('montt', 0.7150738835334778),\n",
       " ('colombiana', 0.7146137952804565),\n",
       " ('santaquin', 0.7144999504089355),\n",
       " ('chile[gallery', 0.713274359703064),\n",
       " ('perurena', 0.713173508644104),\n",
       " ('excepcional', 0.7126935720443726),\n",
       " ('almaden', 0.7126885652542114),\n",
       " ('santisimo', 0.7126781344413757),\n",
       " ('arribart', 0.7125349640846252),\n",
       " ('atonchile', 0.7109968066215515),\n",
       " ('pacifico', 0.7107242345809937),\n",
       " ('camilo', 0.7104405164718628),\n",
       " ('rocuant', 0.7100630402565002),\n",
       " ('colima', 0.7098715305328369),\n",
       " ('casilla', 0.7091021537780762),\n",
       " ('-chilean', 0.7087603211402893),\n",
       " ('madero', 0.7084847688674927),\n",
       " ('guatamalan', 0.7084778547286987),\n",
       " ('veracruz', 0.7082388997077942),\n",
       " ('monteros', 0.7081698179244995),\n",
       " ('chaparros', 0.7081633806228638),\n",
       " ('agenciauno', 0.7076833248138428),\n",
       " ('puert', 0.7074266076087952),\n",
       " ('marica', 0.7071958780288696),\n",
       " ('aguardo', 0.706601619720459),\n",
       " ('nacion', 0.7065248489379883),\n",
       " ('santol', 0.706398606300354),\n",
       " ('mendocino', 0.7058477401733398),\n",
       " ('puerta', 0.7057089805603027),\n",
       " ('colombianas', 0.7054761052131653),\n",
       " ('tiago', 0.7048583030700684),\n",
       " ('bragantino', 0.7045746445655823),\n",
       " ('residencia', 0.7039587497711182),\n",
       " ('plata', 0.7039375305175781),\n",
       " ('paranaense', 0.7037484645843506),\n",
       " ('aracataca', 0.7030707597732544),\n",
       " ('florencia', 0.7028565406799316),\n",
       " ('caraga', 0.7026294469833374),\n",
       " ('rosarito', 0.7023622989654541),\n",
       " ('guatama', 0.7022905349731445),\n",
       " ('paca', 0.70200514793396),\n",
       " ('chile.#spainvschile', 0.7005723118782043),\n",
       " ('chiled', 0.6997271776199341),\n",
       " ('#chileans', 0.6997028589248657),\n",
       " ('placencia', 0.6996564865112305),\n",
       " ('universidade', 0.6995754241943359),\n",
       " ('miraflores', 0.6993354558944702),\n",
       " ('bello', 0.699242115020752),\n",
       " ('olas', 0.6989561319351196),\n",
       " ('ernesto', 0.6989526748657227),\n",
       " ('corona', 0.6988338232040405)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectors.similar_by_word('santiago', topn=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
