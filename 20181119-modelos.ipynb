{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-11-19 16:24:50,356 : loading short urls\n",
      "2018-11-19 16:24:58,057 : loaded 1532034 urls\n",
      "2018-11-19 16:24:58,058 : loading expanded urls\n",
      "2018-11-19 16:25:02,955 : loaded 2214565 urls\n",
      "2018-11-19 16:25:02,956 : cleaning url residual info\n"
     ]
    }
   ],
   "source": [
    "from pymongo import MongoClient\n",
    "from web.mongo_remote_password import user, password, ip\n",
    "from pprint import pprint\n",
    "from collections import Counter, defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from unionfind import UnionFind\n",
    "import logging\n",
    "\n",
    "from gensim.models import KeyedVectors\n",
    "from pathlib import Path\n",
    "import json\n",
    "import spacy\n",
    "from gen_model import gen_model, load_data\n",
    "\n",
    "from pathlib import Path\n",
    "from bson.objectid import ObjectId\n",
    "from sklearn import metrics\n",
    "from collections import namedtuple\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s : %(message)s', level=logging.INFO)\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "#client = MongoClient(f'mongodb://{user}:{password}@{ip}:27017')\n",
    "#db = client.twitter_news\n",
    "\n",
    "topic_labeling_results_path = Path('topic_labeling_results/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-11-19 16:25:45,332 : loading projection weights from /home/mquezada/anchor-text-twitter/data/ft_alltweets_model.vec\n",
      "2018-11-19 16:26:48,987 : loaded (1076139, 100) matrix from /home/mquezada/anchor-text-twitter/data/ft_alltweets_model.vec\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load('en', disable=[\"tagger\", \"parser\", \"ner\"])\n",
    "we = KeyedVectors.load_word2vec_format('/home/mquezada/anchor-text-twitter/data/ft_alltweets_model.vec')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-11-19 12:20:31,020 : load and clean dataset: libya_hotel_tweets.tsv\n",
      "2018-11-19 12:20:31,165 : tweets processed: 26331, ignored: 2309, missing urls: 6341\n",
      "2018-11-19 12:20:31,166 : create list of tweet_ids\n",
      "100%|██████████| 26331/26331 [00:00<00:00, 799445.65it/s]\n",
      "2018-11-19 12:20:31,201 : create pairs (t, u) or (t, t') for each tweet t and url u or replied/retweeted tweet t'\n",
      "2018-11-19 12:20:31,255 : total pairs: 20127, retweets: 13389, quotes: 0, replies: 313 (missing: 131, missing urls: 6341)\n",
      "2018-11-19 12:20:31,255 : applying union-find\n",
      "2018-11-19 12:20:31,411 : total components: 3399\n",
      "2018-11-19 12:20:31,411 : \n",
      "\n",
      "2018-11-19 12:20:31,413 : load and clean dataset: oscar_pistorius_tweets.tsv\n",
      "2018-11-19 12:20:31,894 : tweets processed: 112260, ignored: 955, missing urls: 21807\n",
      "2018-11-19 12:20:31,894 : create list of tweet_ids\n",
      "100%|██████████| 112260/112260 [00:00<00:00, 938171.98it/s]\n",
      "2018-11-19 12:20:32,016 : create pairs (t, u) or (t, t') for each tweet t and url u or replied/retweeted tweet t'\n",
      "2018-11-19 12:20:32,521 : total pairs: 66848, retweets: 28434, quotes: 0, replies: 1611 (missing: 748, missing urls: 21807)\n",
      "2018-11-19 12:20:32,522 : applying union-find\n",
      "2018-11-19 12:20:33,293 : total components: 9641\n",
      "2018-11-19 12:20:33,294 : \n",
      "\n",
      "2018-11-19 12:20:33,304 : load and clean dataset: nepal_tweets.tsv\n",
      "2018-11-19 12:20:36,284 : tweets processed: 503660, ignored: 19144, missing urls: 60632\n",
      "2018-11-19 12:20:36,285 : create list of tweet_ids\n",
      "100%|██████████| 503660/503660 [00:00<00:00, 880235.32it/s]\n",
      "2018-11-19 12:20:36,859 : create pairs (t, u) or (t, t') for each tweet t and url u or replied/retweeted tweet t'\n",
      "2018-11-19 12:20:38,028 : total pairs: 443160, retweets: 468850, quotes: 0, replies: 5203 (missing: 2726, missing urls: 60632)\n",
      "2018-11-19 12:20:38,029 : applying union-find\n",
      "2018-11-19 12:20:44,916 : total components: 22915\n",
      "2018-11-19 12:20:44,916 : \n",
      "\n",
      "2018-11-19 12:20:44,966 : load and clean dataset: libya_hotel_tweets.tsv\n",
      "2018-11-19 12:20:45,143 : tweets processed: 26331, ignored: 2309, missing urls: 6341\n",
      "2018-11-19 12:20:45,144 : create list of tweet_ids\n",
      "100%|██████████| 26331/26331 [00:00<00:00, 785576.12it/s]\n",
      "2018-11-19 12:20:45,180 : create pairs (t, u) or (t, t') for each tweet t and url u or replied/retweeted tweet t'\n",
      "2018-11-19 12:20:45,243 : total pairs: 20054, retweets: 10884, quotes: 0, replies: 142 (missing: 33, missing urls: 6341)\n",
      "2018-11-19 12:20:45,243 : applying union-find\n",
      "2018-11-19 12:20:45,413 : total components: 3381\n",
      "2018-11-19 12:20:45,414 : \n",
      "\n",
      "2018-11-19 12:20:45,417 : load and clean dataset: oscar_pistorius_tweets.tsv\n",
      "2018-11-19 12:20:45,928 : tweets processed: 112260, ignored: 955, missing urls: 21807\n",
      "2018-11-19 12:20:45,929 : create list of tweet_ids\n",
      "100%|██████████| 112260/112260 [00:00<00:00, 1042525.71it/s]\n",
      "2018-11-19 12:20:46,039 : create pairs (t, u) or (t, t') for each tweet t and url u or replied/retweeted tweet t'\n",
      "2018-11-19 12:20:46,219 : total pairs: 66416, retweets: 16966, quotes: 0, replies: 652 (missing: 221, missing urls: 21807)\n",
      "2018-11-19 12:20:46,220 : applying union-find\n",
      "2018-11-19 12:20:46,964 : total components: 9380\n",
      "2018-11-19 12:20:46,965 : \n",
      "\n",
      "2018-11-19 12:20:46,972 : load and clean dataset: nepal_tweets.tsv\n",
      "2018-11-19 12:20:49,751 : tweets processed: 503660, ignored: 19144, missing urls: 60632\n",
      "2018-11-19 12:20:49,752 : create list of tweet_ids\n",
      "100%|██████████| 503660/503660 [00:00<00:00, 1015038.96it/s]\n",
      "2018-11-19 12:20:50,250 : create pairs (t, u) or (t, t') for each tweet t and url u or replied/retweeted tweet t'\n",
      "2018-11-19 12:20:51,121 : total pairs: 442367, retweets: 378601, quotes: 0, replies: 2538 (missing: 854, missing urls: 60632)\n",
      "2018-11-19 12:20:51,122 : applying union-find\n",
      "2018-11-19 12:20:57,818 : total components: 22574\n",
      "2018-11-19 12:20:57,819 : \n",
      "\n",
      "2018-11-19 12:20:57,863 : load and clean dataset: libya_hotel_tweets.tsv\n",
      "2018-11-19 12:20:57,996 : tweets processed: 26331, ignored: 2309, missing urls: 6341\n",
      "2018-11-19 12:20:57,996 : create list of tweet_ids\n",
      "100%|██████████| 26331/26331 [00:00<00:00, 853327.61it/s]\n",
      "2018-11-19 12:20:58,030 : create pairs (t, u) or (t, t') for each tweet t and url u or replied/retweeted tweet t'\n",
      "2018-11-19 12:20:58,086 : total pairs: 19945, retweets: 13389, quotes: 0, replies: 310 (missing: 131, missing urls: 6341)\n",
      "2018-11-19 12:20:58,086 : applying union-find\n",
      "2018-11-19 12:20:58,235 : total components: 3385\n",
      "2018-11-19 12:20:58,236 : \n",
      "\n",
      "2018-11-19 12:20:58,239 : load and clean dataset: oscar_pistorius_tweets.tsv\n",
      "2018-11-19 12:20:59,370 : tweets processed: 112260, ignored: 955, missing urls: 21807\n",
      "2018-11-19 12:20:59,370 : create list of tweet_ids\n",
      "100%|██████████| 112260/112260 [00:00<00:00, 1002261.79it/s]\n",
      "2018-11-19 12:20:59,484 : create pairs (t, u) or (t, t') for each tweet t and url u or replied/retweeted tweet t'\n",
      "2018-11-19 12:20:59,680 : total pairs: 65985, retweets: 28434, quotes: 0, replies: 1611 (missing: 748, missing urls: 21807)\n",
      "2018-11-19 12:20:59,681 : applying union-find\n",
      "2018-11-19 12:21:00,415 : total components: 9337\n",
      "2018-11-19 12:21:00,416 : \n",
      "\n",
      "2018-11-19 12:21:00,425 : load and clean dataset: nepal_tweets.tsv\n",
      "2018-11-19 12:21:03,631 : tweets processed: 503660, ignored: 19144, missing urls: 60632\n",
      "2018-11-19 12:21:03,631 : create list of tweet_ids\n",
      "100%|██████████| 503660/503660 [00:00<00:00, 877893.84it/s]\n",
      "2018-11-19 12:21:04,207 : create pairs (t, u) or (t, t') for each tweet t and url u or replied/retweeted tweet t'\n",
      "2018-11-19 12:21:05,290 : total pairs: 440683, retweets: 468850, quotes: 0, replies: 5194 (missing: 2722, missing urls: 60632)\n",
      "2018-11-19 12:21:05,291 : applying union-find\n",
      "2018-11-19 12:21:12,038 : total components: 22687\n",
      "2018-11-19 12:21:12,039 : \n",
      "\n",
      "2018-11-19 12:21:12,086 : load and clean dataset: libya_hotel_tweets.tsv\n",
      "2018-11-19 12:21:12,223 : tweets processed: 26331, ignored: 2309, missing urls: 6341\n",
      "2018-11-19 12:21:12,223 : create list of tweet_ids\n",
      "100%|██████████| 26331/26331 [00:00<00:00, 867585.93it/s]\n",
      "2018-11-19 12:21:12,255 : create pairs (t, u) or (t, t') for each tweet t and url u or replied/retweeted tweet t'\n",
      "2018-11-19 12:21:12,305 : total pairs: 19945, retweets: 10884, quotes: 0, replies: 133 (missing: 31, missing urls: 6341)\n",
      "2018-11-19 12:21:12,306 : applying union-find\n",
      "2018-11-19 12:21:12,461 : total components: 3385\n",
      "2018-11-19 12:21:12,462 : \n",
      "\n",
      "2018-11-19 12:21:12,465 : load and clean dataset: oscar_pistorius_tweets.tsv\n",
      "2018-11-19 12:21:12,954 : tweets processed: 112260, ignored: 955, missing urls: 21807\n",
      "2018-11-19 12:21:12,954 : create list of tweet_ids\n",
      "100%|██████████| 112260/112260 [00:00<00:00, 1089222.96it/s]\n",
      "2018-11-19 12:21:13,059 : create pairs (t, u) or (t, t') for each tweet t and url u or replied/retweeted tweet t'\n",
      "2018-11-19 12:21:13,206 : total pairs: 65985, retweets: 16966, quotes: 0, replies: 652 (missing: 221, missing urls: 21807)\n",
      "2018-11-19 12:21:13,207 : applying union-find\n",
      "2018-11-19 12:21:13,921 : total components: 9337\n",
      "2018-11-19 12:21:13,922 : \n",
      "\n",
      "2018-11-19 12:21:13,929 : load and clean dataset: nepal_tweets.tsv\n",
      "2018-11-19 12:21:17,411 : tweets processed: 503660, ignored: 19144, missing urls: 60632\n",
      "2018-11-19 12:21:17,412 : create list of tweet_ids\n",
      "100%|██████████| 503660/503660 [00:00<00:00, 994953.02it/s] \n",
      "2018-11-19 12:21:17,920 : create pairs (t, u) or (t, t') for each tweet t and url u or replied/retweeted tweet t'\n",
      "2018-11-19 12:21:18,787 : total pairs: 440683, retweets: 378601, quotes: 0, replies: 2516 (missing: 845, missing urls: 60632)\n",
      "2018-11-19 12:21:18,787 : applying union-find\n",
      "2018-11-19 12:21:25,457 : total components: 22687\n",
      "2018-11-19 12:21:25,458 : \n",
      "\n"
     ]
    }
   ],
   "source": [
    "def gen_model(dataset_name, ignore_wo_url=False, ignore_replies=False):   \n",
    "    event_data, missing_urls_amount = load_data(dataset_name)\n",
    "\n",
    "    ##########\n",
    "    # create set of tweet_ids\n",
    "    # for a given tweet t:\n",
    "    # if t does not have urls: add a tweet_id {t.id}_0\n",
    "    # for each url_i in t: add a tweet_id {t.id}_{i}\n",
    "    # for each url_i in t: add a tweet_id {t.reply_id}_{i}\n",
    "    ##########\n",
    "    tweet_ids = set()\n",
    "    logging.info(\"create list of tweet_ids\")\n",
    "    \n",
    "    for tweet_id, tweet in tqdm(event_data.items(), total=len(event_data)):\n",
    "        added = False\n",
    "        if not tweet.expanded_urls:\n",
    "            if not ignore_wo_url:\n",
    "                tweet_ids.add(f'{tweet_id}_0')\n",
    "                added = True\n",
    "        else:       \n",
    "            for i, url in enumerate(tweet.expanded_urls.values()):\n",
    "                tweet_ids.add(f'{tweet_id}_{i}')\n",
    "                added = True\n",
    "                \n",
    "        if added and tweet.reply_id != 'NULL':\n",
    "            if tweet.reply_id in event_data and not ignore_replies:\n",
    "                for i, url in enumerate(tweet.expanded_urls.values()):\n",
    "                    tweet_ids.add(f'{tweet.reply_id}_{i}')\n",
    "                    \n",
    "    ##########\n",
    "    # for each tweet_id in the set of tweet_ids\n",
    "    # add a pair\n",
    "    ##########\n",
    "    logging.info(\"create pairs (t, u) or (t, t') for each tweet t and url u or replied/retweeted tweet t'\")\n",
    "    replies_amount = 0\n",
    "    retweets_amount = 0\n",
    "    quotes_amount = 0\n",
    "    missing_replies_amount = 0\n",
    "    pairs = []\n",
    "    \n",
    "    for tweet_id in tweet_ids:\n",
    "        frags = tweet_id.split('_')\n",
    "        o_tweet_id = frags[0]\n",
    "        i = int(frags[1])\n",
    "        \n",
    "        tweet = event_data[o_tweet_id]\n",
    "        \n",
    "        url = tweet.expanded_urls.get(i)\n",
    "        if url:\n",
    "            pairs.append((tweet_id, url))\n",
    "        \n",
    "        # retweets ARE considered, due to be exact text copies of the retweeted tweet\n",
    "        if tweet.retweet_id != 'NULL':\n",
    "            retweets_amount += 1\n",
    "        if tweet.quote_id != 'NULL':\n",
    "            quotes_amount += 1\n",
    "        if tweet.reply_id != 'NULL':\n",
    "            replies_amount += 1\n",
    "\n",
    "            if tweet.reply_id in event_data:\n",
    "                if not ignore_replies:\n",
    "                    pairs.append((tweet_id, f'{tweet.reply_id}_{i}'))\n",
    "            else:\n",
    "                missing_replies_amount += 1\n",
    "                \n",
    "    logging.info(f'total pairs: {len(pairs)}, retweets: {retweets_amount}, quotes: {quotes_amount}, replies: {replies_amount} '\n",
    "                 f'(missing: {missing_replies_amount}, missing urls: {missing_urls_amount})')\n",
    "\n",
    "    ##########\n",
    "\n",
    "    \"\"\"\n",
    "        all keys must be the same time (in this case, strings);\n",
    "        unionfind will vectorize operations and will cast everything in the array to the same type,\n",
    "        so if there are integers and strings, it will cast everything to string and comparisons will fail\n",
    "        when calling uf.components().\n",
    "    \"\"\"\n",
    "\n",
    "    logging.info('applying union-find')\n",
    "    uf = UnionFind()\n",
    "    for u, v in pairs:\n",
    "        uf.union(u, v)\n",
    "    logging.info(f'total components: {len(uf.components())}')\n",
    "    logging.info('\\n')\n",
    "\n",
    "    return {\n",
    "        'uf': uf, \n",
    "        'event_data': event_data\n",
    "    }\n",
    "\n",
    "\n",
    "# event_name: (uf, event_data)\n",
    "models = {\n",
    "    'libya': gen_model('libya_hotel_tweets.tsv'),\n",
    "    'pistorius': gen_model('oscar_pistorius_tweets.tsv'),\n",
    "    'nepal': gen_model('nepal_tweets.tsv'),\n",
    "    \n",
    "    'libya_no_url': gen_model('libya_hotel_tweets.tsv', ignore_wo_url=True),\n",
    "    'pistorius_no_url': gen_model('oscar_pistorius_tweets.tsv', ignore_wo_url=True),\n",
    "    'nepal_no_url': gen_model('nepal_tweets.tsv', ignore_wo_url=True),\n",
    "    \n",
    "    'libya_no_rep': gen_model('libya_hotel_tweets.tsv', ignore_replies=True),\n",
    "    'pistorius_no_rep': gen_model('oscar_pistorius_tweets.tsv', ignore_replies=True),\n",
    "    'nepal_no_rep': gen_model('nepal_tweets.tsv', ignore_replies=True),\n",
    "    \n",
    "    'libya_no_url_no_rep': gen_model('libya_hotel_tweets.tsv', ignore_wo_url=True, ignore_replies=True),\n",
    "    'pistorius_no_url_no_rep': gen_model('oscar_pistorius_tweets.tsv', ignore_wo_url=True, ignore_replies=True),\n",
    "    'nepal_no_url_no_rep': gen_model('nepal_tweets.tsv', ignore_wo_url=True, ignore_replies=True)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "libya\n",
      "[1802, 1058, 746, 653, 415, 398, 385, 237, 236, 187, 186, 148, 145, 140, 125]\n",
      "\n",
      "pistorius\n",
      "[2674, 1003, 873, 766, 748, 720, 692, 619, 490, 464, 423, 408, 405, 405, 399]\n",
      "\n",
      "nepal\n",
      "[55309, 45063, 12120, 4439, 3885, 3161, 3067, 2640, 2607, 2482, 2304, 2299, 2246, 2170, 2138]\n",
      "\n",
      "libya_no_url\n",
      "[1801, 1055, 664, 653, 415, 398, 385, 237, 236, 187, 186, 148, 145, 139, 125]\n",
      "\n",
      "pistorius_no_url\n",
      "[2674, 998, 873, 766, 748, 720, 690, 618, 490, 464, 423, 408, 405, 405, 399]\n",
      "\n",
      "nepal_no_url\n",
      "[55249, 45023, 12116, 4439, 3867, 3159, 3063, 2639, 2605, 2482, 2304, 2297, 2246, 2170, 2136]\n",
      "\n",
      "libya_no_rep\n",
      "[1801, 1055, 653, 413, 385, 375, 273, 237, 236, 187, 186, 148, 145, 139, 127]\n",
      "\n",
      "pistorius_no_rep\n",
      "[2674, 998, 832, 766, 748, 720, 690, 618, 490, 464, 423, 408, 405, 405, 399]\n",
      "\n",
      "nepal_no_rep\n",
      "[22269, 14115, 11147, 7947, 6699, 6335, 4688, 4439, 3994, 3936, 3134, 3102, 3061, 2869, 2718]\n",
      "\n",
      "libya_no_url_no_rep\n",
      "[1801, 1055, 653, 413, 385, 375, 273, 237, 236, 187, 186, 148, 145, 139, 127]\n",
      "\n",
      "pistorius_no_url_no_rep\n",
      "[2674, 998, 832, 766, 748, 720, 690, 618, 490, 464, 423, 408, 405, 405, 399]\n",
      "\n",
      "nepal_no_url_no_rep\n",
      "[22269, 14115, 11147, 7947, 6699, 6335, 4688, 4439, 3994, 3936, 3134, 3102, 3061, 2869, 2718]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# size of largest components\n",
    "for _ename, _model in models.items():\n",
    "    print(_ename)\n",
    "    sizes = sorted([len(x) for x in _model['uf'].components()], reverse=True)\n",
    "    print(sizes[:15])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1654"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet_topic = dict()\n",
    "with open('topic_labeling_results/tweet_topic.tsv') as f:\n",
    "    for line in f:\n",
    "        tweet_id, topic_id = line.split('\\t')\n",
    "        tweet_topic[tweet_id] = topic_id[:-1]\n",
    "        \n",
    "len(tweet_topic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# gen docs, compute vectors "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = {}\n",
    "\n",
    "for _ename, _model in models.items():\n",
    "    logging.info(_ename)\n",
    "    \n",
    "    component_key = []\n",
    "    component_values = []\n",
    "    \n",
    "    for component in _model['uf'].components():\n",
    "        for element in component:\n",
    "            if element.startswith('http:'):\n",
    "                component_key.append(element)\n",
    "            else:\n",
    "                tweet = _model['event_data'][element]\n",
    "                component_values.append(tweet.tweet_id)\n",
    "                \n",
    "    if not component_key:\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from word2vec_twitter.word2vecReader import Word2Vec\n",
    "\n",
    "w2v = Word2Vec.load_word2vec_format('word2vec_twitter_model.bin', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('iran', 0.6382494568824768)\n",
      "('afganistan', 0.6247891187667847)\n",
      "('iraq', 0.6207890510559082)\n",
      "('bosnia', 0.6153123378753662)\n",
      "('egypt', 0.6109287738800049)\n",
      "('waziristan', 0.6071416139602661)\n",
      "('kosovo', 0.6057484745979309)\n",
      "('algeria', 0.604259729385376)\n",
      "('rwanda', 0.6036934852600098)\n",
      "('Sirte', 0.6035150289535522)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('#libya', 0.8835678696632385),\n",
       " ('inlibya', 0.8677065372467041),\n",
       " ('liby', 0.8656959533691406),\n",
       " ('nlibya', 0.8640224933624268),\n",
       " ('-libya', 0.8595587611198425),\n",
       " ('libyan', 0.8530803918838501),\n",
       " ('libya\\\\', 0.8520067930221558),\n",
       " ('libyas', 0.8493448495864868),\n",
       " ('libyavoic', 0.8482226133346558),\n",
       " ('libyalibyas', 0.8432244062423706)]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inp = 'libya'\n",
    "\n",
    "for t, s, _ in w2v.most_similar(inp):\n",
    "    print((t, s))\n",
    "    \n",
    "we.most_similar(inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-11-19 16:27:05,245 : precomputing L2-norms of word weight vectors\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('pistorius-', 0.9676041007041931),\n",
       " ('pistoriuss', 0.9662495851516724),\n",
       " ('-pistorius', 0.9560325145721436),\n",
       " ('#pistorius', 0.9548238515853882),\n",
       " ('pistoriu', 0.954453706741333),\n",
       " ('sca.#pistorius', 0.9541462659835815),\n",
       " ('pistori', 0.9444199204444885),\n",
       " ('#pistoriuss', 0.9405568838119507),\n",
       " ('oscarpistorius', 0.9321249723434448),\n",
       " ('cnnpistorius', 0.9316268563270569)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
