{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymongo import MongoClient\n",
    "from bson.objectid import ObjectId\n",
    "import random\n",
    "import logging\n",
    "from datetime import datetime\n",
    "from functools import lru_cache\n",
    "from paths import data_path\n",
    "from pathlib import Path\n",
    "from gensim.models import KeyedVectors\n",
    "from tqdm import tqdm\n",
    "from pprint import pprint\n",
    "import spacy\n",
    "from nltk.corpus import stopwords\n",
    "import pyemd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from sklearn.cluster import KMeans, AgglomerativeClustering\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(format='%(asctime)s : %(message)s', level=logging.INFO)\n",
    "_info = logging.info\n",
    "\n",
    "client = MongoClient('mongodb://localhost:27017')\n",
    "db = client.twitter_news\n",
    "\n",
    "counters = dict()\n",
    "\n",
    "total_events = 3\n",
    "\n",
    "@lru_cache(maxsize=total_events)\n",
    "def get_representatives(event_id):\n",
    "    _info(\"getting representatives\")\n",
    "    representatives = db.representatives.find({'event': ObjectId(event_id)})\n",
    "    return list(representatives)\n",
    "\n",
    "\n",
    "@lru_cache(maxsize=total_events)\n",
    "def get_topics(event_id):\n",
    "    _info(\"getting topics\")\n",
    "    topics = list(db.topics.find({'event': ObjectId(event_id)}))\n",
    "    for t in topics:\n",
    "        if t['topic_name'] == \"Non relevant\":\n",
    "            comodin = t\n",
    "            topics.remove(t)\n",
    "            break\n",
    "    return topics, comodin\n",
    "\n",
    "\n",
    "@lru_cache(maxsize=1)\n",
    "def get_events():\n",
    "    _info(\"getting events\")\n",
    "    events = db.events.find()\n",
    "    return list(events)\n",
    "\n",
    "\n",
    "@lru_cache(maxsize=1)\n",
    "def get_tweets(a=None):\n",
    "    _info('getting all tweets')\n",
    "    all_tweets = db.tweets.find()\n",
    "    return list(all_tweets)\n",
    "\n",
    "\n",
    "@lru_cache(maxsize=1)\n",
    "def get_vectors(path):\n",
    "    _info(f\"loading fasttext vectors from {path}\")\n",
    "    word_vectors = KeyedVectors.load_word2vec_format(path)\n",
    "    return word_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm', tagger=False, entity=False, matcher=False)\n",
    "\n",
    "def hashtag_pipe(doc):\n",
    "    merged_hashtag = False\n",
    "    while True:\n",
    "        for token_index, token in enumerate(doc):\n",
    "            if token.text == '#':\n",
    "                if token.head is not None:\n",
    "                    start_index = token.idx\n",
    "                    end_index = start_index + len(token.head.text) + 1\n",
    "                    if doc.merge(start_index, end_index) is not None:\n",
    "                        merged_hashtag = True\n",
    "                        break\n",
    "        if not merged_hashtag:\n",
    "            break\n",
    "        merged_hashtag = False\n",
    "    return doc\n",
    "\n",
    "nlp.add_pipe(hashtag_pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-06-15 12:34:10,280 : loading fasttext vectors from /home/mquezada/anchor-text-twitter/data/ft_alltweets_model.vec\n",
      "2018-06-15 12:34:10,280 : loading projection weights from /home/mquezada/anchor-text-twitter/data/ft_alltweets_model.vec\n",
      "2018-06-15 12:35:13,385 : loaded (1076139, 100) matrix from /home/mquezada/anchor-text-twitter/data/ft_alltweets_model.vec\n",
      "2018-06-15 12:35:13,386 : getting all tweets\n",
      "2018-06-15 12:35:18,845 : getting events\n",
      "2018-06-15 12:35:18,847 : getting representatives\n",
      "2018-06-15 12:35:19,127 : getting topics\n",
      "100%|██████████| 642251/642251 [00:00<00:00, 1174059.91it/s]\n"
     ]
    }
   ],
   "source": [
    "path = data_path / Path('/home/mquezada/anchor-text-twitter/data/ft_alltweets_model.vec')\n",
    "vectors = get_vectors(path.as_posix())\n",
    "\n",
    "all_tweets = get_tweets()\n",
    "events = get_events()\n",
    "event = events[1]\n",
    "\n",
    "representatives = get_representatives(event['_id'])\n",
    "topics, _ = get_topics(event['_id'])\n",
    "\n",
    "rep_tweet = dict()\n",
    "for t in tqdm(all_tweets):\n",
    "    rep_tweet[t['representative']] = t\n",
    "\n",
    "rep_set = set([r['_id'] for r in representatives])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "82626"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# asociacion entre representativos y tweets\n",
    "\n",
    "tweets_this_event = [t for r, t in rep_tweet.items() if r in rep_set]\n",
    "len(tweets_this_event)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Oscar Pistorius apologizes\n",
      "Oscar Pistorius vomits on court\n",
      "Oscar Pistorius removes his prosthesis\n",
      "Psychiatric evaluation\n",
      "Final arguments\n",
      "Pistorius pledges innocence\n",
      "Paddy Powers\n",
      "Witnesses\n",
      "Police under investigation\n",
      "Interrogatory\n",
      "Shooting in a restaurant\n"
     ]
    }
   ],
   "source": [
    "for topic in topics:\n",
    "    print(topic['topic_name'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El problema con lo siguiente son los topicos: las palabras definen los tweets que se van a obtener con la búsqueda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['cnn', 'world', 'neighbor', 'heard', 'lady', 'scream', 'man', 'heard', 'woman', \"'s\", 'voice', 'night', 'steenkamp', 'died']\n",
      "\n",
      "['oscar', 'pistorius', 'apologizes']\n",
      "0.7109102456922951\n",
      "\n",
      "['oscar', 'pistorius', 'vomits', 'court']\n",
      "0.771266765169569\n",
      "\n",
      "['oscar', 'pistorius', 'removes', 'prosthesis']\n",
      "0.7534386777219071\n",
      "\n",
      "['psychiatric', 'evaluation']\n",
      "0.5569321009896898\n",
      "\n",
      "['final', 'arguments']\n",
      "0.7287406698576708\n",
      "\n",
      "['pistorius', 'pledges', 'innocence']\n",
      "0.7636569244937059\n",
      "\n",
      "['paddy', 'powers']\n",
      "0.6548144737134705\n",
      "\n",
      "['witnesses']\n",
      "0.6771960242053019\n",
      "\n",
      "['police', 'investigation']\n",
      "0.7503976143332373\n",
      "\n",
      "['interrogatory']\n",
      "0.6773885430319018\n",
      "\n",
      "['shooting', 'restaurant']\n",
      "0.7625576741760687\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# convertir topicos en word vectors\n",
    "# convertir tweets del evento (representativos) en vectores\n",
    "## app lowercase\n",
    "## app tokenize\n",
    "## del urls\n",
    "# computar similitud entre topico y tweet\n",
    "\n",
    "rnd_tweet = random.choice(tweets_this_event)\n",
    "text = nlp(rnd_tweet['text'])\n",
    "\n",
    "tweet_tokens = []\n",
    "\n",
    "for token in text:\n",
    "    if token.lower_ in stopwords.words('english') or token.lower_ not in vectors:\n",
    "        continue\n",
    "    tweet_tokens.append(token.lower_)\n",
    "\n",
    "topic_tokens = []\n",
    "for topic in nlp.pipe([t['topic_name'] for t in topics]):\n",
    "    topic_tokens.append([token.lower_ for token in topic \n",
    "                         if token.lower_ in vectors and token.lower_ not in stopwords.words('english')])\n",
    "    \n",
    "print(tweet_tokens)\n",
    "print()\n",
    "for tokens in topic_tokens:\n",
    "    print(tokens)\n",
    "    print(vectors.n_similarity(tweet_tokens, tokens))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Idea: hacer clustering de palabras del vocabulario y samplear tweets a partir de los clusters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 82626/82626 [11:54<00:00, 115.71it/s]\n"
     ]
    }
   ],
   "source": [
    "tweets_tokens = []\n",
    "for _tweet in tqdm(tweets_this_event):\n",
    "    text = nlp(_tweet['text'])\n",
    "\n",
    "    tweet_tokens = []\n",
    "\n",
    "    for token in text:\n",
    "        if token.lower_ in stopwords.words('english') or token.lower_ not in vectors:\n",
    "            continue\n",
    "        tweet_tokens.append(token.lower_)\n",
    "\n",
    "    tweets_tokens.append(tweet_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(preprocessor=lambda x: x, tokenizer=lambda x: x)\n",
    "\n",
    "m = vectorizer.fit_transform(tweets_tokens)\n",
    "m2 = m.transpose()  # represent words instead of documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1, 16831), (13, 6), (14, 4), (8, 4), (10, 3), (7, 2), (12, 2), (2, 2), (19, 2), (3, 1), (17, 1), (11, 1), (5, 1), (4, 1), (6, 1), (18, 1), (0, 1), (15, 1), (9, 1), (16, 1)]\n",
      "\n",
      "['murder']\n",
      "\n",
      "['psychiatric', 'tests']\n",
      "\n",
      "['#newslocker']\n",
      "\n",
      "['evidence']\n",
      "\n",
      "['court']\n",
      "\n",
      "['girlfriend']\n",
      "\n",
      "['cross', 'examination']\n",
      "\n",
      "['evaluation', 'judge', 'mental', 'orders']\n",
      "\n",
      "['prosecutor']\n",
      "\n",
      "['oscar', 'pistorius', 'trial']\n",
      "\n",
      "['begins']\n",
      "\n",
      "['graphic', 'testimony']\n",
      "\n",
      "['brother', 'car', 'carl', 'condition', 'crash', 'critical']\n",
      "\n",
      "['arguments', 'begin', 'closing', 'final']\n",
      "\n",
      "['news']\n",
      "\n",
      "['vomits']\n",
      "\n",
      "[\"'s\"]\n",
      "\n",
      "['live']\n",
      "\n",
      "['reeva', 'steenkamp']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "all_words = np.array(vectorizer.get_feature_names())\n",
    "\n",
    "nc = 20\n",
    "km = KMeans(n_clusters=nc, n_jobs=-1, n_init=30, max_iter=500)\n",
    "km.fit(m2)\n",
    "\n",
    "print(Counter(km.labels_).most_common(nc))\n",
    "print()\n",
    "\n",
    "for i in range(nc):\n",
    "    words = all_words[km.labels_ == i].tolist()\n",
    "    if len(words) < 20:\n",
    "        pprint(words)\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "agglomerative clustering doesn't work well in this case:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-e5978be5e2e1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0magg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAgglomerativeClustering\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_clusters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maffinity\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"cosine\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemory\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"/tmp\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlinkage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"complete\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0magg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/news-model/lib/python3.6/site-packages/sklearn/cluster/hierarchical.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    740\u001b[0m             memory.cache(tree_builder)(X, connectivity,\n\u001b[1;32m    741\u001b[0m                                        \u001b[0mn_clusters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_clusters\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 742\u001b[0;31m                                        **kwargs)\n\u001b[0m\u001b[1;32m    743\u001b[0m         \u001b[0;31m# Cut the tree\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    744\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcompute_full_tree\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/news-model/lib/python3.6/site-packages/sklearn/externals/joblib/memory.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    560\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    561\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 562\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cached_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    563\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    564\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__reduce__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/news-model/lib/python3.6/site-packages/sklearn/externals/joblib/memory.py\u001b[0m in \u001b[0;36m_cached_call\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m    508\u001b[0m                           \u001b[0;34m'directory %s'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    509\u001b[0m                         % (name, argument_hash, output_dir))\n\u001b[0;32m--> 510\u001b[0;31m             \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetadata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    511\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmmap_mode\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    512\u001b[0m                 \u001b[0;31m# Memmap the output at the first call to be consistent with\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/news-model/lib/python3.6/site-packages/sklearn/externals/joblib/memory.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    742\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_verbose\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    743\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mformat_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 744\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    745\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_persist_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    746\u001b[0m         \u001b[0mduration\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/news-model/lib/python3.6/site-packages/sklearn/cluster/hierarchical.py\u001b[0m in \u001b[0;36m_complete_linkage\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    525\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_complete_linkage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    526\u001b[0m     \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'linkage'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'complete'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 527\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mlinkage_tree\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    529\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/news-model/lib/python3.6/site-packages/sklearn/cluster/hierarchical.py\u001b[0m in \u001b[0;36mlinkage_tree\u001b[0;34m(X, connectivity, n_components, n_clusters, linkage, affinity, return_distance)\u001b[0m\n\u001b[1;32m    415\u001b[0m             \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtriu_indices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    416\u001b[0m             \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 417\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhierarchy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinkage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlinkage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetric\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maffinity\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    418\u001b[0m         \u001b[0mchildren_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    419\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/news-model/lib/python3.6/site-packages/scipy/cluster/hierarchy.py\u001b[0m in \u001b[0;36mlinkage\u001b[0;34m(y, method, metric, optimal_ordering)\u001b[0m\n\u001b[1;32m    706\u001b[0m                          \u001b[0;34m'matrix looks suspiciously like an uncondensed '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    707\u001b[0m                          'distance matrix')\n\u001b[0;32m--> 708\u001b[0;31m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdistance\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpdist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetric\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    709\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    710\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"`y` must be 1 or 2 dimensional.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/news-model/lib/python3.6/site-packages/scipy/spatial/distance.py\u001b[0m in \u001b[0;36mpdist\u001b[0;34m(X, metric, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1937\u001b[0m             pdist_fn = getattr(_distance_wrap,\n\u001b[1;32m   1938\u001b[0m                                \"pdist_%s_%s_wrap\" % (metric_name, typ))\n\u001b[0;32m-> 1939\u001b[0;31m             \u001b[0mpdist_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1940\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mdm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1941\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "agg = AgglomerativeClustering(n_clusters=20, affinity=\"cosine\", memory=\"/tmp\", linkage=\"complete\")\n",
    "\n",
    "agg.fit(m2.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(20):\n",
    "    words = all_words[agg.labels_ == i].tolist()\n",
    "    if len(words) < 50:\n",
    "        pprint(words)\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors.similar_by_word('santiago', topn=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
